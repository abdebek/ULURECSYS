
export const  Result = [
  {
    "id": "104868982",
    "title": "Approximate learning with multiple machines.",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Mahendran Velauthapillai",
        "id": "1965046444",
        "org": null
      }
    ],
    "year": 1992,
    "topn_sim": null,
    "score": 0.8818301044570745
  },
  {
    "id": "105302814",
    "title": "Introduction to Machine Learning, by Ethem Alpaydin, MIT Press, 2004, ISBN 0-262-01211-1.",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "G. F. Page",
        "id": "2402621312",
        "org": null
      }
    ],
    "year": 2006,
    "topn_sim": null,
    "score": 0.8360296698661321
  },
  {
    "id": "112543629",
    "title": "Lecture Notes in Machine Learning.",
    "abs": "",
    "n_citation": 3,
    "authors": [
      {
        "name": "Xindong Wu",
        "id": "2664397358",
        "org": null
      }
    ],
    "year": 1994,
    "topn_sim": null,
    "score": 0.8032280233459492
  },
  {
    "id": "114747788",
    "title": "Bayesian Modelling for Machine Learning",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Paul Rippon",
        "id": "2394589378",
        "org": "The University of Newcastle, Australia"
      },
      {
        "name": "Kerrie Mengersen",
        "id": "39766430",
        "org": "The University of Newcastle, Australia"
      }
    ],
    "year": 2005,
    "topn_sim": null,
    "score": 0.8028680605818238
  },
  {
    "id": "1150873173",
    "title": "Information Visualization using Machine Learning",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Gregor Leban",
        "id": "2648467759",
        "org": null
      }
    ],
    "year": 2013,
    "topn_sim": null,
    "score": 0.8024128877796561
  },
  {
    "id": "1167346692",
    "title": "Automatic classification of object code using machine learning",
    "abs": " Recent research has repeatedly shown that machine learning techniques can be applied to either whole files or file fragments to classify them for analysis. We build upon these techniques to show that for samples of un-labeled compiled computer object code, one can apply the same type of analysis to classify important aspects of the code, such as its target architecture and endianess. We show that using simple byte-value histograms we retain enough information about the opcodes within a sample to classify the target architecture with high accuracy, and then discuss heuristic-based features that exploit information within the operands to determine endianess. We introduce a dataset with over 16000 code samples from 20 architectures and experimentally show that by using our features, classifiers can achieve very high accuracy with relatively small sample sizes.",
    "n_citation": 5,
    "authors": [
      {
        "name": "John Clemens",
        "id": "2800047749",
        "org": "Johns Hopkins University Applied Physics Laboratory (JHU/APL), Laurel, MD, USA#TAB#"
      }
    ],
    "year": 2015,
    "topn_sim": null,
    "score": 0.7988571790318728
  },
  {
    "id": "120848234",
    "title": "Combinatorial Machine Learning: A Rough Set Approach",
    "abs": " Decision trees and decision rule systems are widely used in different applicationsas algorithms for problem solving, as predictors, and as a way forknowledge representation. Reducts play key role in the problem of attribute(feature) selection. The aims of this book are (i) the consideration of the setsof decision trees, rules and reducts; (ii) study of relationships among theseobjects; (iii) design of algorithms for construction of trees, rules and reducts;and (iv) obtaining bounds on their complexity. Applications for supervisedmachine learning, discrete optimization, analysis of acyclic programs, faultdiagnosis, and pattern recognition are considered also. This is a mixture ofresearch monograph and lecture notes. It contains many unpublished results.However, proofs are carefully selected to be understandable for students.The results considered in this book can be useful for researchers in machinelearning, data mining and knowledge discovery, especially for those who areworking in rough set theory, test theory and logical analysis of data. The bookcan be used in the creation of courses for graduate students.",
    "n_citation": 35,
    "authors": [
      {
        "name": "Mikhail Moshkov",
        "id": "428297634",
        "org": null
      },
      {
        "name": "Beata Zielosko",
        "id": "2811015557",
        "org": null
      }
    ],
    "year": 2013,
    "topn_sim": null,
    "score": 0.7934975622261842
  },
  {
    "id": "122944010",
    "title": "Construction of model of structured documents based on machine learning",
    "abs": " In this paper we consider the problem of structured document recognition. The document recognition system is proposed. This system incorporates a recognition module based on methods of structured image recognition, a graph document model and a method of document model generalization. The machine learning component makes the process of document model construction easier and less time-consuming.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Sergey Golubev",
        "id": "2567200657",
        "org": "Moscow Institute of Physics and Technology, Dolgoprudny, Russia and ABBYY Software, Moscow, Russia#TAB#"
      }
    ],
    "year": 2011,
    "topn_sim": null,
    "score": 0.7932663790319944
  },
  {
    "id": "126884532",
    "title": "General limitations on machine learning",
    "abs": "",
    "n_citation": 10,
    "authors": [
      {
        "name": "Achim G. Hoffmann",
        "id": "2121065413",
        "org": null
      }
    ],
    "year": 1990,
    "topn_sim": null,
    "score": 0.7836620935527394
  },
  {
    "id": "135262809",
    "title": "The Role of Machine Learning in Information Retrieval Research.",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Hugo Zaragoza",
        "id": "2668979109",
        "org": null
      }
    ],
    "year": 2006,
    "topn_sim": null,
    "score": 0.7796619140433558
  },
  {
    "id": "141361502",
    "title": "A new approach in machine learning.",
    "abs": " In this technical report we presented a novel approach to machine learning. Once the new framework is presented, we will provide a simple and yet very powerful learning algorithm which will be benchmark on various dataset. \r\nThe framework we proposed is based on booleen circuits; more specifically the classifier produced by our algorithm have that form. Using bits and boolean gates instead of real numbers and multiplication enable the the learning algorithm and classifier to use very efficient boolean vector operations. This enable both the learning algorithm and classifier to be extremely efficient. The accuracy of the classifier we obtain with our framework compares very favorably those produced by conventional techniques, both in terms of efficiency and accuracy.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Alain Tapp",
        "id": "2649753353",
        "org": null
      }
    ],
    "year": 2014,
    "topn_sim": null,
    "score": 0.7717503329729939
  },
  {
    "id": "141784485",
    "title": "Data Mining and Machine Learning (Abstract).",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Heikki Mannila",
        "id": "2700080609",
        "org": null
      }
    ],
    "year": 1996,
    "topn_sim": null,
    "score": 0.7596869204541334
  },
  {
    "id": "1483494894",
    "title": "Machine Learning Algorithms Inspired by the Work of Ryszard Spencer Michalski",
    "abs": " In this chapter we first define the field of inductive machine learning and then describe Michalski’s basic AQ algorithm. Next, we describe two of our machine learning algorithms, the CLIP4: a hybrid of rule and decision tree algorithms, and the DataSqeezer: a rule algorithm. The development of the latter two algorithms was inspired to a large degree by Michalski’s seminal paper on inductive machine learning (1969). To many researchers, including the authors, Michalski is a “father” of inductive machine learning, as Łukasiewicz is of multivalued logic (extended much later to fuzzy logic) (Łukasiewicz, 1920), and Pawlak of rough sets (1991). Michalski was the first to work on inductive machine learning algorithms that generate rules, which will be explained via describing his AQ algorithm (1986).",
    "n_citation": 2,
    "authors": [
      {
        "name": "Krzysztof J. Cios",
        "id": "2010675024",
        "org": "Virginia Commonwealth University"
      },
      {
        "name": "Krzysztof J. Cios",
        "id": "2010675024",
        "org": "Virginia Commonwealth University"
      },
      {
        "name": "Łukasz A. Kurgan",
        "id": "2343986398",
        "org": "University of Alberta"
      }
    ],
    "year": 2010,
    "topn_sim": null,
    "score": 0.7592585269645142
  },
  {
    "id": "1484701502",
    "title": "The problem of induction and machine learning",
    "abs": " Are we justified in inferring a general rule from observations that frequently confirm it? This is the usual statement of the problem of induction. The present paper argues that this question is relevant for the understanding of Machine Learning, but insufficient. Research in Machine Learning has prompted another, more fundamental question: the number of possible rules grows exponentially with the size of the examples, and many of them are somehow confirmed by the data - how are we to choose effectively some rules that have good chances of being predictive? We analyze if and how this problem is approached in standard accounts of induction and show the difficulties that are present. Finally, we suggest that the Explanation-based Learning approach and related methods of knowledge intensive induction could be a partial solution to some of these problems, and help understanding the question of valid induction from a new perspective.",
    "n_citation": 3,
    "authors": [
      {
        "name": "Francesco Bergadano",
        "id": "321511767",
        "org": "University of Torino, Torino, Italy#TAB#"
      }
    ],
    "year": 1991,
    "topn_sim": null,
    "score": 0.7585800788793421
  },
  {
    "id": "1485001674",
    "title": "Machine learning, EWSL-91 : European Working Session on Learning, Porto, Portugal, March 6-8, 1991, proceedings",
    "abs": " Abstracting background knowledge for concept learning.- A multistrategy learning approach to domain modeling and knowledge acquisition.- Using plausible explanations to bias empirical generalization in weak theory domains.- The replication problem: A constructive induction approach.- Integrating an explanation-based learning mechanism into a general problem-solver.- Analytical negative generalization and empirical negative generalization are not cumulative: A case study.- Evaluating and changing representation in concept acquisition.- Application of empirical discovery in knowledge acquisition.- Using accuracy in scientific discovery.- KBG : A generator of knowledge bases.- On estimating probabilities in tree pruning.- Rule induction with CN2: Some recent improvements.- On changing continuous attributes into ordered discrete attributes.- A method for inductive cost optimization.- When does overfitting decrease prediction accuracy in induced decision trees and rule sets?.- Semi-naive bayesian classifier.- Description contrasting in incremental concept formation.- System FLORA: Learning from time-varying training sets.- Message-based bucket brigade: An algorithm for the apportionment of credit problem.- Acquiring object-knowledge for learning systems.- Learning nonrecursive definitions of relations with linus.- Extending explanation-based generalization by abstraction operators.- Static learning for an adaptative theorem prover.- Explanation-based generalization and constraint propagation with interval labels.- Learning by explanation of failures.- PANEL : Logic and learnability.- Panel on : Causality and learning.- Seed space and version space: Generalizing from approximations.- Integrating EBL with automatic text analysis.- Abduction for explanation-based learning.- Consistent term mappings, term partitions, and inverse resolution.- Learning by analogical replay in prodigy: First results.- Analogical reasoning for logic programming.- Case-based learning of strategic knowledge.- Learning in distributed systems and multi-agent environments.- Learning to relate terms in a multiple agent environment.- Extending learning to multiple agents: Issues and a model for multi-agent machine learning (MA-ML).- Applications of machine learning: Notes from the panel members.- Evaluation of learning systems : An artificial data-based approach.- Shift of bias in learning from drug compounds: The fleming project.- Learning features by experimentation in chess.- Representation and induction of musical structures for computer assisted composition.- IPSA: Inductive protein structure analysis.- Four stances on knowledge acquisition and machine learning.- Programme of EWSL-91.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Yves Kodratoff",
        "id": "2481380929",
        "org": null
      }
    ],
    "year": 1991,
    "topn_sim": null,
    "score": 0.7499577889414349
  },
  {
    "id": "1493821370",
    "title": "Distributed machine learning",
    "abs": "",
    "n_citation": 10,
    "authors": [
      {
        "name": "Gerhard Weiss",
        "id": "2142969897",
        "org": null
      }
    ],
    "year": 1995,
    "topn_sim": null,
    "score": 0.7497741445928966
  },
  {
    "id": "1494519881",
    "title": "Machine Learning at Scale.",
    "abs": " It takes skill to build a meaningful predictive model even with the abundance of implementations of modern machine learning algorithms and readily available computing resources. Building a model becomes challenging if hundreds of terabytes of data need to be processed to produce the training data set. In a digital advertising technology setting, we are faced with the need to build thousands of such models that predict user behavior and power advertising campaigns in a 24/7 chaotic real-time production environment. As data scientists, we also have to convince other internal departments critical to implementation success, our management, and our customers that our machine learning system works. In this paper, we present the details of the design and implementation of an automated, robust machine learning platform that impacts billions of advertising impressions monthly. This platform enables us to continuously optimize thousands of campaigns over hundreds of millions of users, on multiple continents, against varying performance objectives.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Sergei Izrailev",
        "id": "2657839414",
        "org": null
      },
      {
        "name": "Jeremy M. Stanley",
        "id": "2561114360",
        "org": null
      }
    ],
    "year": 2014,
    "topn_sim": null,
    "score": 0.7487704020764324
  },
  {
    "id": "1494846899",
    "title": "Guest Editors‘ Introduction: On Applied Research in MachineLearning",
    "abs": " Common arguments for including applications papers in the Machine Learning literatureare often based on the papers’ value for advertising success stories and for boosting morale.Forexample,high-proﬁleapplicationscanhelptosecurefundingforfutureresearchandcanhelp to attract high caliber students. However, there is another reason why such papers areof value to the ﬁeld, which is, arguably, even more vital. Application papers are essential inorder for Machine Learning to remain a viable science. They focus research on importantunsolved problems that currently restrict the practical applicability of machine learningmethods.Muchofthe“science”ofMachineLearningisascienceofengineering.",
    "n_citation": 81,
    "authors": [
      {
        "name": "Foster Provost",
        "id": "2158932634",
        "org": "Bell Atlantic Science and Technology, 400 Westchester Avenue, White Plains, New York 10604. E-mail: provost@acm.org#TAB#"
      },
      {
        "name": "Ron Kohavi",
        "id": "2502724635",
        "org": "Data Mining and Visualization, Silicon Graphics Inc., 2011 N. Shoreline Blvd, Mountain View, CA. 94043. E-mail: [email protected]#TAB#"
      }
    ],
    "year": 1998,
    "topn_sim": null,
    "score": 0.7477843612193673
  },
  {
    "id": "1496347545",
    "title": "An Experimental Evaluation of Integrating Machine Learning with Knowledge Acquisition",
    "abs": " Machine learning and knowledge acquisition from experts have distinct capabilities that appear to complement one another. We report a study that demonstrates the integration of these approaches can both improve the accuracy of the developed knowledge base and reduce development time. In addition, we found that users expected the expert systems created through the integrated approach to have higher accuracy than those created without machine learning and rated the integrated approach less difficult to use. They also provided favorable evaluations of both the specific integrated software, a system called The Knowledge Factory, and of the general value of machine learning for knowledge acquisition.",
    "n_citation": 33,
    "authors": [
      {
        "name": "Geoffrey I. Webb",
        "id": "2126304162",
        "org": "School of Computing and Mathematics, Deakin University, Geelong, Victoria 3217, Australia. webb@deakin.edu.au#TAB#"
      },
      {
        "name": "Jason Wells",
        "id": "2148134598",
        "org": "School of Computing and Mathematics, Deakin University, Geelong, Victoria 3217, Australia. wells@deakin.edu.au#TAB#"
      },
      {
        "name": "Zijian Zheng",
        "id": "2152523718",
        "org": "School of Computing and Mathematics, Deakin University, Geelong, Victoria 3217, Australia. zijian@deakin.edu.au#TAB#"
      }
    ],
    "year": 1999,
    "topn_sim": null,
    "score": 0.7449832194893555
  },
  {
    "id": "1496929357",
    "title": "Machine-Learning Research",
    "abs": " Machine-learning research has been making great progress in many directions. This article summarizes four of these directions and discusses some current open problems. The four directions are (1) the improvement of classification accuracy by learning ensembles of classifiers, (2) methods for scaling up supervised learning algorithms, (3) reinforcement learning, and (4) the learning of complex stochastic models.",
    "n_citation": 402,
    "authors": [
      {
        "name": "Thomas G. Dietterich",
        "id": "160031478",
        "org": null
      }
    ],
    "year": 1997,
    "topn_sim": null,
    "score": 0.7424612046157109
  },
  {
    "id": "1503398984",
    "title": "Machine Learning: A Probabilistic Perspective",
    "abs": " Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.",
    "n_citation": 1765,
    "authors": [
      {
        "name": "Kevin P. Murphy",
        "id": "2167731548",
        "org": null
      }
    ],
    "year": 2012,
    "topn_sim": null,
    "score": 0.7407876827489039
  },
  {
    "id": "1506934790",
    "title": "The computational complexity of machine learning",
    "abs": " This thesis is a study of the computational complexity of machine learning from examples in the distribution-free model introduced by L. G. Valiant (V84). In the distribution-free model, a learning algorithm receives positive and negative examples of an unknown target set (or concept) that is chosen from some known class of sets (or concept class). These examples are generated randomly according to a fixed but unknown probability distribution representing Nature, and the goal of the learning algorithm is to infer an hypothesis concept that closely approximates the target concept with respect to the unknown distribution. This thesis is concerned with proving theorems about learning in this formal mathematical model.\r\nWe are interested in the phenomenon of efficient learning in the distribution-free model, in the standard polynomial-time sense. Our results include general tools for determining the polynomial-time learnability of a concept class, an extensive study of efficient learning when errors are present in the examples, and lower bounds on the number of examples required for learning in our model. A centerpiece of the thesis is a series of results demonstrating the computational difficulty of learning a number of well-studied concept classes. These results are obtained by reducing some apparently hard number-theoretic problems from cryptography to the learning problems. The hard-to-learn concept classes include the sets represented by Boolean formulae, deterministic finite automata and a simplified form of neural networks. We also give algorithms for learning powerful concept classes under the uniform distribution, and give equivalences between natural models of efficient learnability.\r\nThis thesis also includes detailed definitions and motivation for the distribution-free model, a chapter discussing past research in this model and related models, and a short list of important open problems.",
    "n_citation": 146,
    "authors": [
      {
        "name": "Michael J. Kearns",
        "id": "2118586410",
        "org": null
      }
    ],
    "year": 1990,
    "topn_sim": null,
    "score": 0.7397425873182144
  },
  {
    "id": "1507957806",
    "title": "A First Course in Machine Learning",
    "abs": " A First Course in Machine Learning covers the core mathematical and statistical techniques needed to understand some of the most popular machine learning algorithms. The algorithms presented span the main problem areas within machine learning: classification, clustering and projection. The text gives detailed descriptions and derivations for a small number of algorithms rather than cover many algorithms in less detail. Referenced throughout the text and available on a supporting website (http://bit.ly/firstcourseml), an extensive collection of MATLAB/Octave scripts enables students to recreate plots that appear in the book and investigate changing model specifications and parameter values. By experimenting with the various algorithms and concepts, students see how an abstract set of equations can be used to solve real problems. Requiring minimal mathematical prerequisites, the classroom-tested material in this text offers a concise, accessible introduction to machine learning. It provides students with the knowledge and confidence to explore the machine learning literature and research specific methods in more detail.",
    "n_citation": 46,
    "authors": [
      {
        "name": "Simon Rogers",
        "id": "2172436156",
        "org": null
      },
      {
        "name": "Mark A. Girolami",
        "id": "2139051431",
        "org": null
      }
    ],
    "year": 2011,
    "topn_sim": null,
    "score": 0.7395806340460735
  },
  {
    "id": "1511623085",
    "title": "Machine Learning as a Tool for Building a Deterministic Parser",
    "abs": " Our goal is to construct a natural language “understanding“ program, which integrates the syntactic/semantic processing. The present article is about syntactic parsing. Of the various algorithms proposed (Winograd), we prefer the deterministic analysis principle (see (Rady) for the justification). In order to recognize the diverse grammatical templates of the French language, the processing rules are necessarily complex.",
    "n_citation": 2,
    "authors": [
      {
        "name": "Gil Francopoulo",
        "id": "198961243",
        "org": "LIMSI-CNRS équipe SABAH"
      }
    ],
    "year": 1986,
    "topn_sim": null,
    "score": 0.7394330344401381
  },
  {
    "id": "1513688849",
    "title": "Real-world learning with Markov logic networks",
    "abs": " Machine learning and data mining systems have achieved many impressive successes, but to become truly widespread they must be able to work with less help from people. This requires automating the data cleaning and integration process, handling multiple types of objects and relations at once, and easily incorporating domain knowledge. In this talk, I describe how we are pursuing these aims using Markov logic networks, a representation that combines first-order logic and probabilistic graphical models. Data from multiple sources is integrated by automatically learning mappings between the objects and terms in them. Rich relational structure is learned using a combination of ILP and statistical techniques. Knowledge is incorporated by viewing logic statements as soft constraints on the models to be learned. Application to a real-world university domain shows our approach to be accurate, efficient, and less labor-intensive than traditional ones.",
    "n_citation": 8,
    "authors": [
      {
        "name": "Pedro M. Domingos",
        "id": "2169012919",
        "org": "University of Washington, Seattle, WA#TAB#"
      }
    ],
    "year": 2004,
    "topn_sim": null,
    "score": 0.7370030216208768
  },
  {
    "id": "1514560266",
    "title": "Deterministic and statistical methods in machine learning : First International Workshop, Sheffield, UK, September 7-10, 2004 : revised lectures",
    "abs": " Object Recognition via Local Patch Labelling.- Multi Channel Sequence Processing.- Bayesian Kernel Learning Methods for Parametric Accelerated Life Survival Analysis.- Extensions of the Informative Vector Machine.- Efficient Communication by Breathing.- Guiding Local Regression Using Visualisation.- Transformations of Gaussian Process Priors.- Kernel Based Learning Methods: Regularization Networks and RBF Networks.- Redundant Bit Vectors for Quickly Searching High-Dimensional Regions.- Bayesian Independent Component Analysis with Prior Constraints: An Application in Biosignal Analysis.- Ensemble Algorithms for Feature Selection.- Can Gaussian Process Regression Be Made Robust Against Model Mismatch?.- Understanding Gaussian Process Regression Using the Equivalent Kernel.- Integrating Binding Site Predictions Using Non-linear Classification Methods.- Support Vector Machine to Synthesise Kernels.- Appropriate Kernel Functions for Support Vector Machine Learning with Sequences of Symbolic Data.- Variational Bayes Estimation of Mixing Coefficients.- A Comparison of Condition Numbers for the Full Rank Least Squares Problem.- SVM Based Learning System for Information Extraction.",
    "n_citation": 1,
    "authors": [
      {
        "name": "Joab R. Winkler",
        "id": "2044723163",
        "org": null
      },
      {
        "name": "Mahesan Niranjan",
        "id": "2133447601",
        "org": null
      },
      {
        "name": "Neil D. Lawrence",
        "id": "1979713447",
        "org": null
      }
    ],
    "year": 2005,
    "topn_sim": null,
    "score": 0.7277549795598267
  },
  {
    "id": "1516899090",
    "title": "Elements of Machine Learning, Pat Langley",
    "abs": "",
    "n_citation": 2,
    "authors": [
      {
        "name": "Michael Luck",
        "id": "2063919281",
        "org": "Department of Computer Science, University of Warwich, Coventry CV4 7AL, United Kingdom (E-mail: mikeluck@dcs.warwick.ac.uk)#TAB#"
      }
    ],
    "year": 1998,
    "topn_sim": null,
    "score": 0.7273287263704025
  },
  {
    "id": "1518029699",
    "title": "Machine learning : ECML 2004 : 15th European Conference on Machine Learning, Pisa, Italy, September 20-24, 2004 : proceedings",
    "abs": " Invited Papers.- Random Matrices in Data Analysis.- Data Privacy.- Breaking Through the Syntax Barrier: Searching with Entities and Relations.- Real-World Learning with Markov Logic Networks.- Strength in Diversity: The Advance of Data Analysis.- Contributed Papers.- Filtered Reinforcement Learning.- Applying Support Vector Machines to Imbalanced Datasets.- Sensitivity Analysis of the Result in Binary Decision Trees.- A Boosting Approach to Multiple Instance Learning.- An Experimental Study of Different Approaches to Reinforcement Learning in Common Interest Stochastic Games.- Learning from Message Pairs for Automatic Email Answering.- Concept Formation in Expressive Description Logics.- Multi-level Boundary Classification for Information Extraction.- An Analysis of Stopping and Filtering Criteria for Rule Learning.- Adaptive Online Time Allocation to Search Algorithms.- Model Approximation for HEXQ Hierarchical Reinforcement Learning.- Iterative Ensemble Classification for Relational Data: A Case Study of Semantic Web Services.- Analyzing Multi-agent Reinforcement Learning Using Evolutionary Dynamics.- Experiments in Value Function Approximation with Sparse Support Vector Regression.- Constructive Induction for Classifying Time Series.- Fisher Kernels for Logical Sequences.- The Enron Corpus: A New Dataset for Email Classification Research.- Margin Maximizing Discriminant Analysis.- Multi-objective Classification with Info-Fuzzy Networks.- Improving Progressive Sampling via Meta-learning on Learning Curves.- Methods for Rule Conflict Resolution.- An Efficient Method to Estimate Labelled Sample Size for Transductive LDA(QDA/MDA) Based on Bayes Risk.- Analyzing Sensory Data Using Non-linear Preference Learning with Feature Subset Selection.- Dynamic Asset Allocation Exploiting Predictors in Reinforcement Learning Framework.- Justification-Based Selection of Training Examples for Case Base Reduction.- Using Feature Conjunctions Across Examples for Learning Pairwise Classifiers.- Feature Selection Filters Based on the Permutation Test.- Sparse Distributed Memories for On-Line Value-Based Reinforcement Learning.- Improving Random Forests.- The Principal Components Analysis of a Graph, and Its Relationships to Spectral Clustering.- Using String Kernels to Identify Famous Performers from Their Playing Style.- Associative Clustering.- Learning to Fly Simple and Robust.- Bayesian Network Methods for Traffic Flow Forecasting with Incomplete Data.- Matching Model Versus Single Model: A Study of the Requirement to Match Class Distribution Using Decision Trees.- Inducing Polynomial Equations for Regression.- Efficient Hyperkernel Learning Using Second-Order Cone Programming.- Effective Voting of Heterogeneous Classifiers.- Convergence and Divergence in Standard and Averaging Reinforcement Learning.- Document Representation for One-Class SVM.- Naive Bayesian Classifiers for Ranking.- Conditional Independence Trees.- Exploiting Unlabeled Data in Content-Based Image Retrieval.- Population Diversity in Permutation-Based Genetic Algorithm.- Simultaneous Concept Learning of Fuzzy Rules.- Posters.- SWITCH: A Novel Approach to Ensemble Learning for Heterogeneous Data.- Estimating Attributed Central Orders.- Batch Reinforcement Learning with State Importance.- Explicit Local Models: Towards \"Optimal\" Optimization Algorithms.- An Intelligent Model for the Signorini Contact Problem in Belt Grinding Processes.- Cluster-Grouping: From Subgroup Discovery to Clustering.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Jean-François Boulicaut",
        "id": "1971530415",
        "org": null
      },
      {
        "name": "Floriana Esposito",
        "id": "2122401555",
        "org": null
      },
      {
        "name": "Fosca Giannotti",
        "id": "2612469383",
        "org": null
      },
      {
        "name": "Dino Pedreschi",
        "id": "2429417668",
        "org": null
      }
    ],
    "year": 2004,
    "topn_sim": null,
    "score": 0.7270339793158476
  },
  {
    "id": "1518876726",
    "title": "Advances in Machine Learning II: Dedicated to the memory of Professor Ryszard S. Michalski",
    "abs": " This is the second volume of a large two-volume editorial project we wish to dedicate to the memory of the late Professor Ryszard S. Michalski who passed away in 2007. He was one of the fathers of machine learning, an exciting and relevant, both from the practical and theoretical points of view, area in modern computer science and information technology. His research career started in the mid-1960s in Poland, in the Institute of Automation, Polish Academy of Sciences in Warsaw, Poland. He left for the USA in 1970, and since then had worked there at various universities, notably, at the University of Illinois at Urbana Champaign and finally, until his untimely death, at George Mason University. We, the editors, had been lucky to be able to meet and collaborate with Ryszard for years, indeed some of us knew him when he was still in Poland. After he started working in the USA, he was a frequent visitor to Poland, taking part at many conferences until his death. We had also witnessed with a great personal pleasure honors and awards he had received over the years, notably when some years ago he was elected Foreign Member of the Polish Academy of Sciences among some top scientists and scholars from all over the world, including Nobel prize winners. Professor Michalskis research results influenced very strongly the development of machine learning, data mining, and related areas. Also, he inspired many established and younger scholars and scientists all over the world. We feel very happy that so many top scientists from all over the world agreed to pay the last tribute to Professor Michalski by writing papers in their areas of research. These papers will constitute the most appropriate tribute to Professor Michalski, a devoted scholar and researcher. Moreover, we believe that they will inspire many newcomers and younger researchers in the area of broadly perceived machine learning, data analysis and data mining. The papers included in the two volumes, Machine Learning I and Machine Learning II, cover diverse topics, and various aspects of the fields involved. For convenience of the potential readers, we will now briefly summarize the contents of the particular chapters.",
    "n_citation": 1,
    "authors": [
      {
        "name": "Jacek Koronacki",
        "id": "2634875213",
        "org": null
      },
      {
        "name": "Zbigniew W. Ras",
        "id": "2059569764",
        "org": null
      },
      {
        "name": "Sławomir T. Wierzchoń",
        "id": "2777389367",
        "org": null
      },
      {
        "name": "Janusz Kacprzyk",
        "id": "2719998871",
        "org": null
      }
    ],
    "year": 2009,
    "topn_sim": null,
    "score": 0.7249374014122693
  },
  {
    "id": "1519824474",
    "title": "Programming by Demonstration: A Machine Learning Approach to Support Skill Acquisiton for Robots",
    "abs": " Programming by Demonstration (PbD) is a programming method that allows to add new functionalities to a system by simply showing the desired task or skill in form of few examples. In the domain of robotics this paradigm offers the potential to reduce the complexity of robot task programming and to make programming more ”natural”. In case of programming an assembly task PbD allows with the help of a video or a laser camera and a data glove the automatic generation the necessary robot program for the assembly task. In addition, the demonstration of the task with few different assembly situations and strategies may achieve a generalized assembly function for all possible variants of the class. In order to realize such a PbD system at least two major problems have to be solved. First, the sensor data trace of a demonstration has to be interpreted and transformed into a high-level situation-action representation. This task is not yet well understood nor solved in general. Second, if a generalization is required, induction algorithms must be applied to the sensor data trace, to find the most general user-intended robot function from only few examples. In this paper mainly the second problem is focused. The described experimental PbD environment consists of an industrial robot, a 6D space mouse used as input device, and some sensors. Various data can be recorded during a demonstration for further processing in the PbD system implemented on a workstation. The objective is to exploit the possibilities of integrating learning and clustering algorithms for automated robot programming. In particular it is investigated how human interaction with the PbD system as well as user-initiated dialogs can support inductive learning to acquire generalized assembly programs and skills.",
    "n_citation": 4,
    "authors": [
      {
        "name": "Rüdiger Dillmann",
        "id": "1305265972",
        "org": "University of Karlsruhe"
      },
      {
        "name": "Holger Friedrich",
        "id": "2490961943",
        "org": "University of Karlsruhe"
      }
    ],
    "year": 1996,
    "topn_sim": null,
    "score": 0.7245167248453834
  },
  {
    "id": "1520129216",
    "title": "Advanced lectures on machine learning : ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, Tübingen, Germany, August 4-16, 2003 : revised lectures",
    "abs": " An Introduction to Pattern Classification.- Some Notes on Applied Mathematics for Machine Learning.- Bayesian Inference: An Introduction to Principles and Practice in Machine Learning.- Gaussian Processes in Machine Learning.- Unsupervised Learning.- Monte Carlo Methods for Absolute Beginners.- Stochastic Learning.- to Statistical Learning Theory.- Concentration Inequalities.",
    "n_citation": 1,
    "authors": [
      {
        "name": "Olivier Bousquet",
        "id": "2042048789",
        "org": null
      },
      {
        "name": "Ulrike von Luxburg",
        "id": "251023228",
        "org": null
      },
      {
        "name": "Gunnar Rätsch",
        "id": "168172700",
        "org": null
      }
    ],
    "year": 2004,
    "topn_sim": null,
    "score": 0.7234560969618937
  },
  {
    "id": "1526180263",
    "title": "Machine Learning: From Theory to Applications - Cooperative Research at Siemens and MIT",
    "abs": " Strategic directions in machine learning.- Training a 3-node neural network is NP-complete.- Cryptographic limitations on learning Boolean formulae and finite automata.- Inference of finite automata using homing sequences.- Adaptive search by learning from incomplete explanations of failures.- Learning of rules for fault diagnosis in power supply networks.- Cross references are features.- The schema mechanism.- L-ATMS: A tight integration of EBL and the ATMS.- Massively parallel symbolic induction of protein structure/function relationships.- Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks.- Phoneme discrimination using connectionist networks.- Behavior-based learning to control IR oven heating: Preliminary investigations.- Trellis codes, receptive fields, and fault tolerant, self-repairing neural networks.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Stephen José Hanson",
        "id": "2257745696",
        "org": null
      },
      {
        "name": "Werner Remmele",
        "id": "2504490444",
        "org": null
      },
      {
        "name": "Ronald L. Rivest",
        "id": "695545146",
        "org": null
      }
    ],
    "year": 1993,
    "topn_sim": null,
    "score": 0.7233186567142489
  },
  {
    "id": "1527106129",
    "title": "Machine Learning : ECML 2006 : 17th European Conference on Machine Learning, Berlin, Germany, September 18-22, 2006 : proceedings",
    "abs": " Invited Talks.- On Temporal Evolution in Data Streams.- The Future of CiteSeer: CiteSeerx.- Learning to Have Fun.- Winning the DARPA Grand Challenge.- Challenges of Urban Sensing.- Long Papers.- Learning in One-Shot Strategic Form Games.- A Selective Sampling Strategy for Label Ranking.- Combinatorial Markov Random Fields.- Learning Stochastic Tree Edit Distance.- Pertinent Background Knowledge for Learning Protein Grammars.- Improving Bayesian Network Structure Search with Random Variable Aggregation Hierarchies.- Sequence Discrimination Using Phase-Type Distributions.- Languages as Hyperplanes: Grammatical Inference with String Kernels.- Toward Robust Real-World Inference: A New Perspective on Explanation-Based Learning.- Fisher Kernels for Relational Data.- Evaluating Misclassifications in Imbalanced Data.- Improving Control-Knowledge Acquisition for Planning by Active Learning.- PAC-Learning of Markov Models with Hidden State.- A Discriminative Approach for the Retrieval of Images from Text Queries.- TildeCRF: Conditional Random Fields for Logical Sequences.- Unsupervised Multiple-Instance Learning for Functional Profiling of Genomic Data.- Bayesian Learning of Markov Network Structure.- Approximate Policy Iteration for Closed-Loop Learning of Visual Tasks.- Task-Driven Discretization of the Joint Space of Visual Percepts and Continuous Actions.- EM Algorithm for Symmetric Causal Independence Models.- Deconvolutive Clustering of Markov States.- Patching Approximate Solutions in Reinforcement Learning.- Fast Variational Inference for Gaussian Process Models Through KL-Correction.- Bandit Based Monte-Carlo Planning.- Bayesian Learning with Mixtures of Trees.- Transductive Gaussian Process Regression with Automatic Model Selection.- Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.- Why Is Rule Learning Optimistic and How to Correct It.- Automatically Evolving Rule Induction Algorithms.- Bayesian Active Learning for Sensitivity Analysis.- Mixtures of Kikuchi Approximations.- Boosting in PN Spaces.- Prioritizing Point-Based POMDP Solvers.- Graph Based Semi-supervised Learning with Sharper Edges.- Margin-Based Active Learning for Structured Output Spaces.- Skill Acquisition Via Transfer Learning and Advice Taking.- Constant Rate Approximate Maximum Margin Algorithms.- Batch Classification with Applications in Computer Aided Diagnosis.- Improving the Ranking Performance of Decision Trees.- Multiple-Instance Learning Via Random Walk.- Localized Alternative Cluster Ensembles for Collaborative Structuring.- Distributional Features for Text Categorization.- Subspace Metric Ensembles for Semi-supervised Clustering of High Dimensional Data.- An Adaptive Kernel Method for Semi-supervised Clustering.- To Select or To Weigh: A Comparative Study of Model Selection and Model Weighing for SPODE Ensembles.- Ensembles of Nearest Neighbor Forecasts.- Short Papers.- Learning Process Models with Missing Data.- Case-Based Label Ranking.- Cascade Evaluation of Clustering Algorithms.- Making Good Probability Estimates for Regression.- Fast Spectral Clustering of Data Using Sequential Matrix Compression.- An Information-Theoretic Framework for High-Order Co-clustering of Heterogeneous Objects.- Efficient Inference in Large Conditional Random Fields.- A Kernel-Based Approach to Estimating Phase Shifts Between Irregularly Sampled Time Series: An Application to Gravitational Lenses.- Cost-Sensitive Decision Tree Learning for Forensic Classification.- The Minimum Volume Covering Ellipsoid Estimation in Kernel-Defined Feature Spaces.- Right of Inference: Nearest Rectangle Learning Revisited.- Reinforcement Learning for MDPs with Constraints.- Efficient Non-linear Control Through Neuroevolution.- Efficient Prediction-Based Validation for Document Clustering.- On Testing the Missing at Random Assumption.- B-Matching for Spectral Clustering.- Multi-class Ensemble-Based Active Learning.- Active Learning with Irrelevant Examples.- Classification with Support Hyperplanes.- (Agnostic) PAC Learning Concepts in Higher-Order Logic.- Evaluating Feature Selection for SVMs in High Dimensions.- Revisiting Fisher Kernels for Document Similarities.- Scaling Model-Based Average-Reward Reinforcement Learning for Product Delivery.- Robust Probabilistic Calibration.- Missing Data in Kernel PCA.- Exploiting Extremely Rare Features in Text Categorization.- Efficient Large Scale Linear Programming Support Vector Machines.- An Efficient Approximation to Lookahead in Relational Learners.- Improvement of Systems Management Policies Using Hybrid Reinforcement Learning.- Diversified SVM Ensembles for Large Data Sets.- Dynamic Integration with Random Forests.- Bagging Using Statistical Queries.- Guiding the Search in the NO Region of the Phase Transition Problem with a Partial Subsumption Test.- Spline Embedding for Nonlinear Dimensionality Reduction.- Cost-Sensitive Learning of SVM for Ranking.- Variational Bayesian Dirichlet-Multinomial Allocation for Exponential Family Mixtures.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Johannes Fürnkranz",
        "id": "52482323",
        "org": null
      },
      {
        "name": "Tobias Scheffer",
        "id": "2609429555",
        "org": null
      },
      {
        "name": "Myra Spiliopoulou",
        "id": "2672394148",
        "org": null
      }
    ],
    "year": 2006,
    "topn_sim": null,
    "score": 0.7201927325698124
  },
  {
    "id": "1527773884",
    "title": "A prototypical approach to machine learning",
    "abs": " This paper presents an overview of a research programme on machine learning which is based on the fundamental process of categorization.\r\n\r\nA structure of a computer model designed to achieve categorization is outlined and the knowledge representational forms and developmental learning associated with this approach are discussed.",
    "n_citation": 3,
    "authors": [
      {
        "name": "R. I. Phelps",
        "id": "2234872167",
        "org": "Brunei University#TAB#"
      },
      {
        "name": "Peter B. Musgrove",
        "id": "2684333872",
        "org": "Polytechnic of the South Bank#TAB#"
      }
    ],
    "year": 1985,
    "topn_sim": null,
    "score": 0.717332803799009
  },
  {
    "id": "1530263659",
    "title": "Machine Learning (Panel).",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Yves Kodratoff",
        "id": "2404931811",
        "org": null
      }
    ],
    "year": 1986,
    "topn_sim": null,
    "score": 0.715373119583171
  },
  {
    "id": "1530558387",
    "title": "Learning from Positive Data",
    "abs": " Gold showed in 1967 that not even regular grammars can be exactly identified from positive examples alone. Since it is known that children learn natural grammars almost exclusively from positives examples, Gold's result has been used as a theoretical support for Chomsky's theory of innate human linguistic abilities. In this paper new results are presented which show that within a Bayesian framework not only grammars, but also logic programs are learnable with arbitrarily low expected error from positive examples only. In addition, we show that the upper bound for expected error of a learner which maximises the Bayes' posterior probability when learning from positive examples is within a small additive term of one which does the same from a mixture of positive and negative examples. An Inductive Logic Programming implementation is described which avoids the pitfalls of greedy search by global optimisation of this function during the local construction of individual clauses of the hypothesis. Results of testing this implementation on artificially-generated data-sets are reported. These results are in agreement with the theoretical predictions.",
    "n_citation": 154,
    "authors": [
      {
        "name": "Stephen Muggleton",
        "id": "735181462",
        "org": "Oxford University Computing Laboratory"
      }
    ],
    "year": 1996,
    "topn_sim": null,
    "score": 0.7142575095137724
  },
  {
    "id": "1532499468",
    "title": "Machine Learning : ECML 2001 : 12th European Conference on Machine Learning, Freiburg, Germany, September 5-7, 2001 : proceedings",
    "abs": " Regular Papers.- An Axiomatic Approach to Feature Term Generalization.- Lazy Induction of Descriptions for Relational Case-Based Learning.- Estimating the Predictive Accuracy of a Classifier.- Improving the Robustness and Encoding Complexity of Behavioural Clones.- A Framework for Learning Rules from Multiple Instance Data.- Wrapping Web Information Providers by Transducer Induction.- Learning While Exploring: Bridging the Gaps in the Eligibility Traces.- A Reinforcement Learning Algorithm Applied to Simplified Two-Player Texas Hold'em Poker.- Speeding Up Relational Reinforcement Learning through the Use of an Incremental First Order Decision Tree Learner.- Analysis of the Performance of AdaBoost.M2 for the Simulated Digit-Recognition-Example.- Iterative Double Clustering for Unsupervised and Semi-supervised Learning.- On the Practice of Branching Program Boosting.- A Simple Approach to Ordinal Classification.- Fitness Distance Correlation of Neural Network Error Surfaces: A Scalable, Continuous Optimization Problem.- Extraction of Recurrent Patterns from Stratified Ordered Trees.- Understanding Probabilistic Classifiers.- Efficiently Determining the Starting Sample Size for Progressive Sampling.- Using Subclasses to Improve Classification Learning.- Learning What People (Don't) Want.- Towards a Universal Theory of Artificial Intelligence Based on Algorithmic Probability and Sequential Decisions.- Convergence and Error Bounds for Universal Prediction of Nonbinary Sequences.- Consensus Decision Trees: Using Consensus Hierarchical Clustering for Data Relabelling and Reduction.- Learning of Variability for Invariant Statistical Pattern Recognition.- The Evaluation of Predictive Learners: Some Theoretical and Empirical Results.- An Evolutionary Algorithm for Cost-Sensitive Decision Rule Learning.- A Mixture Approach to Novelty Detection Using Training Data with Outliers.- Applying the Bayesian Evidence Framework to ?-Support Vector Regression.- DQL: A New Updating Strategy for Reinforcement Learning Based on Q-Learning.- A Language-Based Similarity Measure.- Backpropagation in Decision Trees for Regression.- Comparing the Bayes and Typicalness Frameworks.- Symbolic Discriminant Analysis for Mining Gene Expression Patterns.- Social Agents Playing a Periodical Policy.- Learning When to Collaborate among Learning Agents.- Building Committees by Clustering Models Based on Pairwise Similarity Values.- Second Order Features for Maximising Text Classification Performance.- Importance Sampling Techniques in Neural Detector Training.- Induction of Qualitative Trees.- Text Categorization Using Transductive Boosting.- Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing.- Using Domain Knowledge on Population Dynamics Modeling for Equation Discovery.- Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL.- A Unified Framework for Evaluation Metrics in Classification Using Decision Trees.- Improving Term Extraction by System Combination Using Boosting.- Classification on Data with Biased Class Distribution.- Discovering Admissible Simultaneous Equation Models from Observed Data.- Discovering Strong Principles of Expressive Music Performance with the PLCG Rule Learning Strategy.- Proportional k-Interval Discretization for Naive-Bayes Classifiers.- Using Diversity in Preparing Ensembles of Classifiers Based on Different Feature Subsets to Minimize Generalization Error.- Geometric Properties of Naive Bayes in Nominal Domains.- Invited Papers.- Support Vectors for Reinforcement Learning.- Combining Discrete Algorithmic and Probabilistic Approaches in Data Mining.- Statistification or Mystification? The Need for Statistical Thought in Visual Data Mining.- The Musical Expression Project: A Challenge for Machine Learning and Knowledge Discovery.- Scalability, Search, and Sampling: From Smart Algorithms to Active Discovery.",
    "n_citation": 1,
    "authors": [
      {
        "name": "Luc De Raedt",
        "id": "189137728",
        "org": null
      },
      {
        "name": "Peter A. Flach",
        "id": "1814273096",
        "org": null
      }
    ],
    "year": 2001,
    "topn_sim": null,
    "score": 0.7139284037566928
  },
  {
    "id": "1532854728",
    "title": "Crafting Papers on Machine Learning",
    "abs": "",
    "n_citation": 50,
    "authors": [
      {
        "name": "Pat Langley",
        "id": "2811443521",
        "org": null
      }
    ],
    "year": 2000,
    "topn_sim": null,
    "score": 0.7098506027810251
  },
  {
    "id": "1534569928",
    "title": "Towards More Collaboration Between Machine Learning Systems and their Users",
    "abs": " This article investigates a way to deepen collaboration between Machine Learning Systems (MLS) and their users through the generation of explanations. More precisely, it focuses on the advises that may be given for helping the user during the evaluation of the MLS results and their correction. This is illustrated through the system EILP, an explanatory interface that supports the user during these tasks.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Jean-Marc Gabriel",
        "id": "2342374624",
        "org": "Sherpa project (INRIA Rhône-Alpes)"
      }
    ],
    "year": 1997,
    "topn_sim": null,
    "score": 0.7086229317729705
  },
  {
    "id": "1534926980",
    "title": "Machine learning : ECML 2002 : 13th European Conference on Machine Learning, Helsinki, Finland, August 19-23, 2002 : proceedings",
    "abs": " Contributed Papers.- Convergent Gradient Ascent in General-Sum Games.- Revising Engineering Models: Combining Computational Discovery with Knowledge.- Variational Extensions to EM and Multinomial PCA.- Learning and Inference for Clause Identification.- An Empirical Study of Encoding Schemes and Search Strategies in Discovering Causal Networks.- Variance Optimized Bagging.- How to Make AdaBoost.M1 Work for Weak Base Classifiers by Changing Only One Line of the Code.- Sparse Online Greedy Support Vector Regression.- Pairwise Classification as an Ensemble Technique.- RIONA: A Classifier Combining Rule Induction and k-NN Method with Automated Selection of Optimal Neighbourhood.- Using Hard Classifiers to Estimate Conditional Class Probabilities.- Evidence that Incremental Delta-Bar-Delta Is an Attribute-Efficient Linear Learner.- Scaling Boosting by Margin-Based Inclusion of Features and Relations.- Multiclass Alternating Decision Trees.- Possibilistic Induction in Decision-Tree Learning.- Improved Smoothing for Probabilistic Suffix Trees Seen as Variable Order Markov Chains.- Collaborative Learning of Term-Based Concepts for Automatic Query Expansion.- Learning to Play a Highly Complex Game from Human Expert Games.- Reliable Classifications with Machine Learning.- Robustness Analyses of Instance-Based Collaborative Recommendation.- iBoost: Boosting Using an instance-Based Exponential Weighting Scheme.- Towards a Simple Clustering Criterion Based on Minimum Length Encoding.- Class Probability Estimation and Cost-Sensitive Classification Decisions.- On-Line Support Vector Machine Regression.- Q-Cut-Dynamic Discovery of Sub-goals in Reinforcement Learning.- A Multistrategy Approach to the Classification of Phases in Business Cycles.- A Robust Boosting Algorithm.- Case Exchange Strategies in Multiagent Learning.- Inductive Confidence Machines for Regression.- Macro-Operators in Multirelational Learning: A Search-Space Reduction Technique.- Propagation of Q-values in Tabular TD(?).- Transductive Confidence Machines for Pattern Recognition.- Characterizing Markov Decision Processes.- Phase Transitions and Stochastic Local Search in k-Term DNF Learning.- Discriminative Clustering: Optimal Contingency Tables by Learning Metrics.- Boosting Density Function Estimators.- Ranking with Predictive Clustering Trees.- Support Vector Machines for Polycategorical Classification.- Learning Classification with Both Labeled and Unlabeled Data.- An Information Geometric Perspective on Active Learning.- Stacking with an Extended Set of Meta-level Attributes and MLR.- Invited Papers.- Finding Hidden Factors Using Independent Component Analysis.- Reasoning with Classifiers.- A Kernel Approach for Learning from almost Orthogonal Patterns.- Learning with Mixture Models: Concepts and Applications.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Tapio Elomma",
        "id": "1217841870",
        "org": null
      },
      {
        "name": "Heikki Mannila",
        "id": "310734946",
        "org": null
      },
      {
        "name": "Hannu Toivonen",
        "id": "2250270171",
        "org": null
      }
    ],
    "year": 2002,
    "topn_sim": null,
    "score": 0.7074367370666089
  },
  {
    "id": "1549998098",
    "title": "Machine Learning: An Algorithmic Perspective",
    "abs": " Written in an easily accessible style, this book provides the ideal blend of theory and practical, applicable knowledge. It covers neural networks, graphical models, reinforcement learning, evolutionary algorithms, dimensionality reduction methods, and the important area of optimization. It treads the fine line between adequate academic rigor and overwhelming students with equations and mathematical concepts. The author includes examples based on widely available datasets and practical and theoretical problems to test understanding and application of the material. The book describes algorithms with code examples backed up by a website that provides working implementations in Python.",
    "n_citation": 263,
    "authors": [
      {
        "name": "Stephen Marsland",
        "id": "2893027530",
        "org": null
      }
    ],
    "year": 2009,
    "topn_sim": null,
    "score": 0.7032126884967075
  },
  {
    "id": "1552228685",
    "title": "Machine learning : ECML-94 : European Conference on Machine Learning, Catania, Italy, April 6-8, 1994 : proceedings",
    "abs": " Industrial applications of ML: Illustrations for the KAML dilemma and the CBR dream.- Knowledge representation in machine learning.- Inverting implication with small training sets.- A context similarity measure.- Incremental learning of control knowledge for nonlinear problem solving.- Characterizing the applicability of classification algorithms using meta-level learning.- Inductive learning of characteristic concept descriptions from small sets of classified examples.- FOSSIL: A robust relational learner.- A multistrategy learning system and its integration into an interactive floorplanning tool.- Bottom-up induction of oblivious read-once decision graphs.- Estimating attributes: Analysis and extensions of RELIEF.- BMWk revisited generalization and formalization of an algorithm for detecting recursive relations in term sequences.- An analytic and empirical comparison of two methods for discovering probabilistic causal relationships.- Sample PAC-learnability in model inference.- Averaging over decision stumps.- Controlling constructive induction in CIPF: An MDL approach.- Using constraints to building version spaces.- On the utility of predicate invention in inductive logic programming.- Learning problem-solving concepts by reflecting on problem solving.- Existence and nonexistence of complete refinement operators.- A hybrid nearest-neighbor and nearest-hyperrectangle algorithm.- Automated knowledge acquisition for Prospector-like expert systems.- On the role of machine learning in knowledge-based control.- Discovering dynamics with genetic programming.- A geometric approach to feature selection.- Identifying unrecognizable regular languages by queries.- Intensional learning of logic programs.- Partially isomorphic generalization and analogical reasoning.- Learning from recursive, tree structured examples.- Concept formation in complex domains.- An algorithm for learning hierarchical classifiers.- Learning belief network structure from data under causal insufficiency.- Cost-sensitive pruning of decision trees.- An instance-based learning method for databases: An information theoretic approach.- Early screening for gastric cancer using machine learning techniques.- DP1: Supervised and unsupervised clustering.- Using machine learning techniques to interpret results from discrete event simulation.- Flexible integration of multiple learning methods into a problem solving architecture.- Concept sublattices.- The piecewise linear classifier DIPOL92.- Complexity of computing generalized VC-dimensions.- Learning relations without closing the world.- Properties of Inductive Logic Programming in function-free Horn logic.- Representing biases for Inductive Logic Programming.- Biases and their effects in Inductive Logic Programming.- Inductive learning of normal clauses.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Francesco Bergadano",
        "id": "2672330748",
        "org": null
      },
      {
        "name": "Luc De Raedt",
        "id": "189137728",
        "org": null
      }
    ],
    "year": 1994,
    "topn_sim": null,
    "score": 0.7027616956848277
  },
  {
    "id": "1554326155",
    "title": "Machine Learning : proceedings of the Nineteenth International Conference (ICML 2002), University of New South Wales, Sydney, Australia, July 8-12, 2002",
    "abs": " Proceedings of the annual International Conferences on Machine Learning, 1988-present. Current volume: ICML 2002: 19th International Conference on Machine Learning. Submissions are expected that describe empirical, theoretical, and cognitive-modeling research in all areas of machine learning. Submissions that present algorithms for novel learning tasks, interdisciplinary research involving machine learning, or innovative applications of machine learning techniques to challenging, real-world problems are especially encouraged.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Claude Sammut",
        "id": "2081312962",
        "org": null
      },
      {
        "name": "Achim Hoffmann",
        "id": "2675726431",
        "org": null
      }
    ],
    "year": 2002,
    "topn_sim": null,
    "score": 0.7026628390832812
  },
  {
    "id": "1555244713",
    "title": "Elements of machine learning",
    "abs": " Elements of Machine Learning by Pat Langley Preface 1. An overview of machine learning 1.1 The science of machine learning 1.2 Nature of the environment 1.3 Nature of representation and performance 1.4 Nature of the learning component 1.5 Five paradigms for machine learning 1.6 Summary of the chapter 2. The induction of logical conjunctions 2.1 General issues in logical induction 2.2 Nonincremental induction of logical conjunctions 2.3 Heuristic induction of logical conjunctions 2.4 Incremental induction of logical conjunctions 2.5 Incremental hill climbing for logical conjunctions 2.6 Genetic algorithms for logical concept induction 2.7 Summary of the chapter 3. The induction of threshold concepts 3.1 General issues for threshold concepts 3.2 Induction of criteria tables 3.3 Induction of linear threshold units 3.4 Induction of spherical threshold units 3.5 Summary of the chapter 4. The induction of competitive concepts 4.1 Instance-based learning 4.2 Learning probabilistic concept descriptions 4.3 Summary of the chapter 5. The construction of decision lists 5.1 General issues in disjunctive concept induction 5.2 Nonincremental learning using separate and conquer 5.3 Incremental induction using separate and conquer 5.4 Induction of decision lists through exceptions 5.5 Induction of competitive disjunctions 5.6 Instance-storing algorithms 5.7 Complementary beam search for disjunctive concepts 5.8 Summary of the chapter 6. Revision and extension of inference networks 6.1 General issues surrounding inference network 6.2 Extending an incomplete inference network 6.3 Inducing specialized concepts with inference networks 6.4 Revising an incorrect inference network 6.5 Network construction and term generation 6.6 Summary of the chapter 7. The formation of concept hierarchies 7.1 General issues concerning concept hierarchies 7.2 Nonincremental divisive formation of hierarchies 7.3 Incremental formation of concept hierarchies 7.4 Agglomerative formation of concept hierarchies 7.5 Variations on hierarchies into other structures 7.7 Summary of the chapter 8. Other issues in concept induction 8.1 Overfitting and pruning 8.2 Selecting useful features 8.3 Induction for numeric prediction 8.4 Unsupervised concept induction 8.5 Inducing relational concepts 8.6 Handling missing features 8.7 Summary of the chapter 9. The formation of transition networks 9.1 General issues for state-transition networks 9.2 Constructing finite-state transition networks 9.3 Forming recursive transition networks 9.4 Learning rules and networks for prediction 9.5 Summary of the chapter 10. The acquisition of search-control knowledge 10.1 General issues in search control 10.2 Reinforcement learning 10.3 Learning state-space heuristics from solution traces 10.4 Learning control knowledge for problem reduction 10.5 Learning control knowledge for means-ends analysis 10.6 The utility of search-control knowledge 10.7 Summary of the chapter 11. The formation of macro-operators 11.1 General issues related to macro-operators 11.2 The creation of simple macro-operators 11.3 The formation of flexible macro-operators 11.4 Problem solving by analogy 11.5 The utility of macro-operators 11.6 Summary of the chapter 12. Prospects for machine learning 12.1 Additional areas of machine learning 12.2 Methodological trends in machine learning 12.3 The future of machine learning References Index",
    "n_citation": 391,
    "authors": [
      {
        "name": "Pat Langley",
        "id": "2811443521",
        "org": "Institute for the Study of Learning and Expertise"
      }
    ],
    "year": 1995,
    "topn_sim": null,
    "score": 0.7020875063121731
  },
  {
    "id": "1555424171",
    "title": "Statistical Learning of Arbitrary Computable Classifiers",
    "abs": " Statistical learning theory chiefly studies restricted\r\nhypothesis classes, particularly those with finite\r\nVapnik-Chervonenkis (VC) dimension. The fundamental\r\nquantity of interest is the sample complexity:\r\nthe number of samples required to learn to\r\na specified level of accuracy. Here we consider\r\nlearning over the set of all computable labeling\r\nfunctions. Since the VC-dimension is infinite and\r\na priori (uniform) bounds on the number of samples\r\nare impossible, we let the learning algorithm\r\ndecide when it has seen sufficient samples to have\r\nlearned. We first show that learning in this setting\r\nis indeed possible, and develop a learning algorithm.\r\nWe then show, however, that bounding sample\r\ncomplexity independently of the distribution is\r\nimpossible. Notably, this impossibility is entirely\r\ndue to the requirement that the learning algorithm\r\nbe computable, and not due to the statistical nature\r\nof the problem.",
    "n_citation": 0,
    "authors": [
      {
        "name": "David Soloveichik",
        "id": "103618433",
        "org": "California Institute of Technology"
      }
    ],
    "year": 2008,
    "topn_sim": null,
    "score": 0.7015185981061506
  },
  {
    "id": "1558381678",
    "title": "Machine learning : ECML 2000 : 11th European Conference on Machine Learning, Barcelona, Catalonia, Spain, May 31-June 2, 2000 : proceedings",
    "abs": " Invited Papers.- Beyond Occam's Razor: Process-Oriented Evaluation.- The Representation Race - Preprocessing for Handling Time Phenomena.- Contributed Papers.- Short-Term Profiling for a Case-Based Reasoning Recommendation System.- K-SVCR. A Multi-class Support Vector Machine.- Learning Trading Rules with Inductive Logic Programming.- Improving Knowledge Discovery Using Domain Knowledge in Unsupervised Learning.- Exploiting Classifier Combination for Early Melanoma Diagnosis Support.- A Comparison of Ranking Methods for Classification Algorithm Selection.- Hidden Markov Models with Patterns and Their Application to Integrated Circuit Testing.- Comparing Complete and Partial Classification for Identifying Latently Dissatisfied Customers.- Wrapper Generation via Grammar Induction.- Diversity versus Quality in Classification Ensembles Based on Feature Selection.- Minimax TD-Learning with Neural Nets in a Markov Game.- Boosting Applied to Word Sense Disambiguation.- A Multiple Model Cost-Sensitive Approach for Intrusion Detection.- Value Miner: A Data Mining Environment for the Calculation of the Customer Lifetime Value with Application to the Automotive Industry.- Investigation and Reduction of Discretization Variance in Decision Tree Induction.- Asymmetric Co-evolution for Imperfect-Information Zero-Sum Games.- A Machine Learning Approach to Workflow Management.- The Utilization of Context Signals in the Analysis of ABR Potentials by Application of Neural Networks.- Complexity Approximation Principle and Rissanen's Approach to Real-Valued Parameters.- Handling Continuous-Valued Attributes in Decision Tree with Neural Network Modeling.- Learning Context-Free Grammars with a Simplicity Bias.- Partially Supervised Text Classification: Combining Labeled and Unlabeled Documents Using an EM-like Scheme.- Toward an Explanatory Similarity Measure for Nearest-Neighbor Classification.- Relative Unsupervised Discretization for Regression Problems.- Metric-Based Inductive Learning Using Semantic Height Functions.- Error Analysis of Automatic Speech Recognition Using Principal Direction Divisive Partitioning.- A Study on the Performance of Large Bayes Classifier.- Dynamic Discretization of Continuous Values from Time Series.- Using a Symbolic Machine Learning Tool to Refine Lexico-syntactic Patterns.- Measuring Performance when Positives Are Rare: Relative Advantage versus Predictive Accuracy - A Biological Case-Study.- Mining TCP/IP Traffic for Network Intrusion Detection by Using a Distributed Genetic Algorithm.- Learning Patterns of Behavior by Observing System Events.- Dimensionality Reduction through Sub-space Mapping for Nearest Neighbour Algorithms.- Nonparametric Regularization of Decision Trees.- An Efficient and Effective Procedure for Updating a Competence Model for Case-Based Reasoners.- Layered Learning.- Problem Decomposition for Behavioural Cloning.- Dynamic Feature Selection in Incremental Hierarchical Clustering.- On the Boosting Pruning Problem.- An Empirical Study of MetaCost Using Boosting Algorithms.- Clustered Partial Linear Regression.- Knowledge Discovery from Very Large Databases Using Frequent Concept Lattices.- Some Improvements on Event-Sequence Temporal Region Methods.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Ramón López de Mántaras",
        "id": "2130487846",
        "org": null
      },
      {
        "name": "Enric Plaza",
        "id": "2643653952",
        "org": null
      }
    ],
    "year": 2000,
    "topn_sim": null,
    "score": 0.7012469436073796
  },
  {
    "id": "1558551749",
    "title": "Integrating Models of Knowledge and Machine Learning",
    "abs": " We propose a framework allowing a real integration of Machine Learning and Knowledge acquisition. This paper shows how the input of a Machine Learning system can be mapped to the model of expertise as it is used in KADS methodology. The notion of learning bias will play a central role. We shall see that parts of it can be identified to what KADS's people call the inference and the task models. Doing this conceptual mapping, we give a semantics to most of the inputs of Machine Learning programs in terms of knowledge acquisition models. The ENIGME system which implements this work will be presented\r\n\r\nWe thank the whole machine learning and knowledge acquisition team at the LAFORIA, particularly Karine Causse and Bernard Le Roux. The research reported here was carried out in the course of the VITAL project P5365 partially funded by the ESPRIT II program of the Commission of the European Communities.",
    "n_citation": 8,
    "authors": [
      {
        "name": "Jean-Gabriel Ganascia",
        "id": "2779758332",
        "org": "LAFORIA-CNRS, Université Pierre et Marie Curie, Paris#TAB#"
      },
      {
        "name": "Jérôme Thomas",
        "id": "2436833221",
        "org": "ONERA, DMI, GIA, Chatillon and LAFORIA-CNRS, Université Pierre et Marie Curie, Paris#TAB#"
      },
      {
        "name": "Philippe Laublet",
        "id": "51425068",
        "org": "ONERA, DMI, GIA, Chatillon#TAB#"
      }
    ],
    "year": 1993,
    "topn_sim": null,
    "score": 0.7008067902033898
  },
  {
    "id": "1559611879",
    "title": "In Memoriam: Arthur Samuel: Pioneer in Machine Learning",
    "abs": " Arthur Samuel (1901-1990) was a pioneer of artificial intelligence research. From 1949 through the late 1960s, he did the best work in making computers learn from their expe-rience. His vehicle for this work was the game of checkers.",
    "n_citation": 6,
    "authors": [
      {
        "name": "John McCarthy",
        "id": "2121245618",
        "org": null
      },
      {
        "name": "Edward A. Feigenbaum",
        "id": "2086686599",
        "org": null
      }
    ],
    "year": 1990,
    "topn_sim": null,
    "score": 0.6991228189012333
  },
  {
    "id": "1559828774",
    "title": "Theory-Practice Interplay in Machine Learning --- Emerging Theoretical Challenges",
    "abs": " Theoretical analysis has played a major role in some of the most prominent practical successes of statistical machine learning. However, mainstream machine learning theory assumes some strong simplifying assumptions which are often unrealistic. In the past decade, the practice of machine learning has led to the development of various heuristic paradigms that answer the needs of a vastly growing range of applications. Many useful such paradigms fall beyond the scope of the currently available analysis. Will theory play a similar pivotal role in the newly emerging sub areas of machine learning?\r\n\r\nIn this talk, I will survey some such application-motivated theoretical challenges. In particular, I will discuss recent developments in the theoretical analysis of semi-supervised learning, multi-task learning, \"learning to learn\", privacy-preserving learning and more.",
    "n_citation": 1,
    "authors": [
      {
        "name": "Shai Ben-David",
        "id": "2151264347",
        "org": "University of Waterloo, Canada#TAB#"
      }
    ],
    "year": 2009,
    "topn_sim": null,
    "score": 0.6979729470718865
  },
  {
    "id": "1560585634",
    "title": "Verifying the fully \"Laplacianised\" posterior Naïve Bayesian approach and more",
    "abs": " Background\r\nIn a recent paper, Mussa, Mitchell and Glen (MMG) have mathematically demonstrated that the “Laplacian Corrected Modified Naive Bayes” (LCMNB) algorithm can be viewed as a variant of the so-called Standard Naive Bayes (SNB) scheme, whereby the role played by absence of compound features in classifying/assigning the compound to its appropriate class is ignored. MMG have also proffered guidelines regarding the conditions under which this omission may hold. Utilising three data sets, the present paper examines the validity of these guidelines in practice. The paper also extends MMG’s work and introduces a new version of the SNB classifier: “Tapered Naive Bayes” (TNB). TNB does not discard the role of absence of a feature out of hand, nor does it fully consider its role. Hence, TNB encapsulates both SNB and LCMNB.",
    "n_citation": 2,
    "authors": [
      {
        "name": "Hamse Y. Mussa",
        "id": "40238850",
        "org": "Centre for Molecular Informatics"
      },
      {
        "name": "David Marcus",
        "id": "2799162210",
        "org": "European Bioinformatics Institute (EMBL-EBI), European Molecular Biology Laboratory, Cambridge, UK"
      },
      {
        "name": "John B. O. Mitchell",
        "id": "2158294628",
        "org": "EaStCHEM School of Chemistry and Biomedical Sciences Research Complex, University of St Andrews, St Andrews, UK"
      },
      {
        "name": "Robert C. Glen",
        "id": "42269114",
        "org": "Department of Chemistry, Centre for Molecular Informatics, Cambridge, UK"
      }
    ],
    "year": 2015,
    "topn_sim": null,
    "score": 0.6973735452440233
  },
  {
    "id": "1561620708",
    "title": "Book Review: \"Machine Learning: A Theoretical Approach\".",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Lisa Hellerstein",
        "id": "2056505999",
        "org": null
      }
    ],
    "year": 1993,
    "topn_sim": null,
    "score": 0.6972361007402088
  },
  {
    "id": "1567012231",
    "title": "Classes of kernels for machine learning: a statistics perspective",
    "abs": " In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.",
    "n_citation": 304,
    "authors": [
      {
        "name": "Marc G. Genton",
        "id": "2124723576",
        "org": "Department of Statistics, North Carolina State University, Raleigh, NC#TAB#"
      }
    ],
    "year": 2002,
    "topn_sim": null,
    "score": 0.6956604397773412
  },
  {
    "id": "1568000229",
    "title": "Machine learning: an integrated framework and its applications",
    "abs": " Knowledge representation relating logical expressions to concept examples in a database framework induction explanation-based learning an integrated system knowledge refinement a classifier using the learned knowledge application to speech/image recognition application to fault diagnosis.",
    "n_citation": 16,
    "authors": [
      {
        "name": "Francesco Bergadano",
        "id": "321511767",
        "org": "Univ. of Torino, Turin, Italy#TAB#"
      },
      {
        "name": "Attilio Giordana",
        "id": "2039763176",
        "org": "Univ. of Torino, Turin, Italy#TAB#"
      },
      {
        "name": "Lorenza Saitta",
        "id": "2505446148",
        "org": "Univ. of Trento, Trento, Italy#TAB#"
      }
    ],
    "year": 1991,
    "topn_sim": null,
    "score": 0.6948961170274164
  },
  {
    "id": "1568521728",
    "title": "Advanced lectures on machine learning : Machine Learning Summer School 2002, Canberra, Australia, February 11-22, 2002 : revised lectures",
    "abs": " A Few Notes on Statistical Learning Theory.- A Short Introduction to Learning with Kernels.- Bayesian Kernel Methods.- An Introduction to Boosting and Leveraging.- An Introduction to Reinforcement Learning Theory: Value Function Methods.- Learning Comprehensible Theories from Structured Data.- Algorithms for Association Rules.- Online Learning of Linear Classifiers.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Shahar Mendelson",
        "id": "1981906831",
        "org": null
      },
      {
        "name": "Alexander J. Smola",
        "id": "1972291593",
        "org": null
      }
    ],
    "year": 2003,
    "topn_sim": null,
    "score": 0.6944916300947327
  },
  {
    "id": "1568604274",
    "title": "Types and classes of machine learning and data mining",
    "abs": " The notion of a statistical model, as inferred and used in statistics, machine learning and data mining, is examined from a semantic point of view. Data types and type-classes for models are developed that allow models to be manipulated in a type-safe yet flexible way. The programming language Haskell-98, with its system of polymorphic types and type-classes, is used as the meta-language for this exercise so one of the by-products is a running program.",
    "n_citation": 9,
    "authors": [
      {
        "name": "Lloyd Allison",
        "id": "2166429794",
        "org": "School of Computer Science and Software Engineering, Monash University, Clayton, Victoria, Australia#TAB#"
      }
    ],
    "year": 2003,
    "topn_sim": null,
    "score": 0.6938481797534696
  },
  {
    "id": "1570520305",
    "title": "Advances in machine learning : First Asian Conference on Machine Learning, ACML 2009, Nanjing, China, November 2-4, 2009 ; proceedings",
    "abs": " Keynote and Invited Talks.- Machine Learning and Ecosystem Informatics: Challenges and Opportunities.- Density Ratio Estimation: A New Versatile Tool for Machine Learning.- Transfer Learning beyond Text Classification.- Regular Papers.- Improving Adaptive Bagging Methods for Evolving Data Streams.- A Hierarchical Face Recognition Algorithm.- Estimating Likelihoods for Topic Models.- Conditional Density Estimation with Class Probability Estimators.- Linear Time Model Selection for Mixture of Heterogeneous Components.- Max-margin Multiple-Instance Learning via Semidefinite Programming.- A Reformulation of Support Vector Machines for General Confidence Functions.- Robust Discriminant Analysis Based on Nonparametric Maximum Entropy.- Context-Aware Online Commercial Intention Detection.- Feature Selection via Maximizing Neighborhood Soft Margin.- Accurate Probabilistic Error Bound for Eigenvalues of Kernel Matrix.- Community Detection on Weighted Networks: A Variational Bayesian Method.- Averaged Naive Bayes Trees: A New Extension of AODE.- Automatic Choice of Control Measurements.- Coupled Metric Learning for Face Recognition with Degraded Images.- Cost-Sensitive Boosting: Fitting an Additive Asymmetric Logistic Regression Model.- On Compressibility and Acceleration of Orthogonal NMF for POMDP Compression.- Building a Decision Cluster Forest Model to Classify High Dimensional Data with Multi-classes.- Query Selection via Weighted Entropy in Graph-Based Semi-supervised Classification.- Learning Algorithms for Domain Adaptation.- Mining Multi-label Concept-Drifting Data Streams Using Dynamic Classifier Ensemble.- Learning Continuous-Time Information Diffusion Model for Social Behavioral Data Analysis.- Privacy-Preserving Evaluation of Generalization Error and Its Application to Model and Attribute Selection.- Coping with Distribution Change in the Same Domain Using Similarity-Based Instance Weighting.- Monte-Carlo Tree Search in Poker Using Expected Reward Distributions.- Injecting Structured Data to Generative Topic Model in Enterprise Settings.- Weighted Nonnegative Matrix Co-Tri-Factorization for Collaborative Prediction.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Zhi-Hua Zhou",
        "id": "2286237009",
        "org": null
      },
      {
        "name": "Takashi Washio",
        "id": "2701710145",
        "org": null
      }
    ],
    "year": 2009,
    "topn_sim": null,
    "score": 0.6931926325726779
  },
  {
    "id": "1572284792",
    "title": "Applications of machine learning",
    "abs": " During the last 10 years, machine learning has been successfully applied. Most often, the applications are confidential. Therefore, only few publications about real world applications exist. In this paper, an overview of machine learning applications is given with their scenarios. Some typical applications are described. Then, future directions of machine learning applications are proposed. It is argued that machine learning is now mature enough to be incorporated into standard systems as well as algorithms. The integration of learning modules into database and retrieval systems is one of the trends. Another trend is to automatically select an appropriate learning tool out of a toolbox. The third trend, which is even more challenging, no longer requires a distinguished learning module, but offers methods of machine learning to be applied by programmers in their regular system development. Software engineers of the future can use inductive techniques as they now use message passing, for instance. Then, any program can be enhanced by some learning ability.",
    "n_citation": 5,
    "authors": [
      {
        "name": "Katharina Morik",
        "id": "2070565061",
        "org": "University Dortmund"
      }
    ],
    "year": 1992,
    "topn_sim": null,
    "score": 0.6925237308705571
  },
  {
    "id": "1573203660",
    "title": "Machine Learning (Panel).",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Douglas B. Lenat",
        "id": "2635083109",
        "org": null
      }
    ],
    "year": 1986,
    "topn_sim": null,
    "score": 0.6902236869035857
  },
  {
    "id": "1574135120",
    "title": "Machine learning with seriated graphs",
    "abs": " The aim in this paper is to show how the problem of learning the class-structure and modes of structural variation in sets of graphs can be solved by converting the graphs to strings. We commence by showing how the problem of converting graphs to strings, or seriation, can be solved using semi-definite programming (SDP). This is a convex optimisation procedure that has recently found widespread use in computer vision for problems including image segmentation and relaxation labelling. We detail the representation needed to cast the graph-seriation problem in a matrix setting so that it can be solved using SDP. We show how the strings delivered by our method can be used for graph-clustering and the construction of graph eigenspaces.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Hang Yu",
        "id": "2308914943",
        "org": "Department of Computer Science, University of York, York, UK#TAB#"
      },
      {
        "name": "Edwin R. Hancock",
        "id": "2047018924",
        "org": "Department of Computer Science, University of York, York, UK#TAB#"
      }
    ],
    "year": 2005,
    "topn_sim": null,
    "score": 0.6885692372288953
  },
  {
    "id": "1574585351",
    "title": "A report on the ECAId98 conference from the machine learning perspective",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Evgueni N. Smirnov",
        "id": "2122405168",
        "org": "MATRIKS, Department of Computer Science, Maastricht University, P.O. Box 616, 6200 MD Maastricht, The Netherlands E&dash"
      }
    ],
    "year": 1998,
    "topn_sim": null,
    "score": 0.6884572625761889
  },
  {
    "id": "1578816018",
    "title": "Efficiently constructing relational features from background knowledge for inductive machine learning",
    "abs": " Most existing inductive learning systems form concept descriptions in propositional languages from vectors of basic features. However, many concepts are characterized by the relationships of individual examples to general domain knowledge. We describe a system that constructs relational terms efficiently to augment the description language of standard inductive systems. In our approach, examples and domain knowledge are combined into an inheritance network, and a form of spreading activation is used to find relevant relational terms. Since there is an equivalence between inheritance networks and relational databases, this yields a method for exploring tables in the database and finding relevant relationships among data to characterize concepts. We also describe the implementation of a prototype system on the CM-2 parallel computer and some experiments with large data sets.",
    "n_citation": 16,
    "authors": [
      {
        "name": "John M. Aronis",
        "id": "60325257",
        "org": "Computer Science Department, University of Pittsburgh, Pittsburgh, PA#TAB#"
      },
      {
        "name": "Foster Provost",
        "id": "2158932634",
        "org": "Computer Science Department, University of Pittsburgh, Pittsburgh, PA#TAB#"
      }
    ],
    "year": 1994,
    "topn_sim": null,
    "score": 0.6878509252836078
  },
  {
    "id": "1583505058",
    "title": "Machine learning : ECML 2005 : 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005 : proceedings",
    "abs": " Invited Talks.- Data Analysis in the Life Sciences - Sparking Ideas -.- Machine Learning for Natural Language Processing (and Vice Versa?).- Statistical Relational Learning: An Inductive Logic Programming Perspective.- Recent Advances in Mining Time Series Data.- Focus the Mining Beacon: Lessons and Challenges from the World of E-Commerce.- Data Streams and Data Synopses for Massive Data Sets (Invited Talk).- Long Papers.- Clustering and Metaclustering with Nonnegative Matrix Decompositions.- A SAT-Based Version Space Algorithm for Acquiring Constraint Satisfaction Problems.- Estimation of Mixture Models Using Co-EM.- Nonrigid Embeddings for Dimensionality Reduction.- Multi-view Discriminative Sequential Learning.- Robust Bayesian Linear Classifier Ensembles.- An Integrated Approach to Learning Bayesian Networks of Rules.- Thwarting the Nigritude Ultramarine: Learning to Identify Link Spam.- Rotational Prior Knowledge for SVMs.- On the LearnAbility of Abstraction Theories from Observations for Relational Learning.- Beware the Null Hypothesis: Critical Value Tables for Evaluating Classifiers.- Kernel Basis Pursuit.- Hybrid Algorithms with Instance-Based Classification.- Learning and Classifying Under Hard Budgets.- Training Support Vector Machines with Multiple Equality Constraints.- A Model Based Method for Automatic Facial Expression Recognition.- Margin-Sparsity Trade-Off for the Set Covering Machine.- Learning from Positive and Unlabeled Examples with Different Data Distributions.- Towards Finite-Sample Convergence of Direct Reinforcement Learning.- Infinite Ensemble Learning with Support Vector Machines.- A Kernel Between Unordered Sets of Data: The Gaussian Mixture Approach.- Active Learning for Probability Estimation Using Jensen-Shannon Divergence.- Natural Actor-Critic.- Inducing Head-Driven PCFGs with Latent Heads: Refining a Tree-Bank Grammar for Parsing.- Learning (k,l)-Contextual Tree Languages for Information Extraction.- Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method.- MCMC Learning of Bayesian Network Models by Markov Blanket Decomposition.- On Discriminative Joint Density Modeling.- Model-Based Online Learning of POMDPs.- Simple Test Strategies for Cost-Sensitive Decision Trees.- -Likelihood and -Updating Algorithms: Statistical Inference in Latent Variable Models.- An Optimal Best-First Search Algorithm for Solving Infinite Horizon DEC-POMDPs.- Ensemble Learning with Supervised Kernels.- Using Advice to Transfer Knowledge Acquired in One Reinforcement Learning Task to Another.- A Distance-Based Approach for Action Recommendation.- Multi-armed Bandit Algorithms and Empirical Evaluation.- Annealed Discriminant Analysis.- Network Game and Boosting.- Model Selection in Omnivariate Decision Trees.- Bayesian Network Learning with Abstraction Hierarchies and Context-Specific Independence.- Short Papers.- Learning to Complete Sentences.- The Huller: A Simple and Efficient Online SVM.- Inducing Hidden Markov Models to Model Long-Term Dependencies.- A Similar Fragments Merging Approach to Learn Automata on Proteins.- Nonnegative Lagrangian Relaxation of K-Means and Spectral Clustering.- Severe Class Imbalance: Why Better Algorithms Aren't the Answer.- Approximation Algorithms for Minimizing Empirical Error by Axis-Parallel Hyperplanes.- A Comparison of Approaches for Learning Probability Trees.- Counting Positives Accurately Despite Inaccurate Classification.- Optimal Stopping and Constraints for Diffusion Models of Signals with Discontinuities.- An Evolutionary Function Approximation Approach to Compute Prediction in XCSF.- Using Rewards for Belief State Updates in Partially Observable Markov Decision Processes.- Active Learning in Partially Observable Markov Decision Processes.- Machine Learning of Plan Robustness Knowledge About Instances.- Two Contributions of Constraint Programming to Machine Learning.- A Clustering Model Based on Matrix Approximation with Applications to Cluster System Log Files.- Detecting Fraud in Health Insurance Data: Learning to Model Incomplete Benford's Law Distributions.- Efficient Case Based Feature Construction.- Fitting the Smallest Enclosing Bregman Ball.- Similarity-Based Alignment and Generalization.- Fast Non-negative Dimensionality Reduction for Protein Fold Recognition.- Mode Directed Path Finding.- Classification with Maximum Entropy Modeling of Predictive Association Rules.- Classification of Ordinal Data Using Neural Networks.- Independent Subspace Analysis on Innovations.- On Applying Tabling to Inductive Logic Programming.- Learning Models of Relational Stochastic Processes.- Error-Sensitive Grading for Model Combination.- Strategy Learning for Reasoning Agents.- Combining Bias and Variance Reduction Techniques for Regression Trees.- Analysis of Generic Perceptron-Like Large Margin Classifiers.- Multimodal Function Optimizing by a New Hybrid Nonlinear Simplex Search and Particle Swarm Algorithm.",
    "n_citation": 0,
    "authors": [
      {
        "name": "João Gama",
        "id": "2113857198",
        "org": null
      },
      {
        "name": "Rui Camacho",
        "id": "2114828967",
        "org": null
      },
      {
        "name": "Pavel Brazdil",
        "id": "396705451",
        "org": null
      },
      {
        "name": "Alípio Mário Jorge",
        "id": "2165617838",
        "org": null
      },
      {
        "name": "Luís Torgo",
        "id": "1973271071",
        "org": null
      }
    ],
    "year": 2005,
    "topn_sim": null,
    "score": 0.6861493462243027
  },
  {
    "id": "1583837637",
    "title": "Proceedings of the 29th International Conference on Machine Learning (ICML-12)",
    "abs": " This is an index to the papers that appear in the Proceedings of the 29th International Conference on Machine Learning (ICML-12). The conference was held in Edinburgh, Scotland, June 27th - July 3rd, 2012.",
    "n_citation": 0,
    "authors": [
      {
        "name": "John Langford",
        "id": "2232397101",
        "org": null
      },
      {
        "name": "Joelle Pineau",
        "id": "2092672596",
        "org": "Editors"
      }
    ],
    "year": 2012,
    "topn_sim": null,
    "score": 0.6860396812050843
  },
  {
    "id": "1584509007",
    "title": "Strategic Directions in Machine Learning",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Stephen José Hanson",
        "id": "2257745696",
        "org": "Learning Systems Lab. Siemens"
      },
      {
        "name": "Werner Remmele",
        "id": "2504490444",
        "org": "Siemens AG Corporate Research and Technology"
      },
      {
        "name": "Ronald L. Rivest",
        "id": "695545146",
        "org": "MIT Laboratory for Computer Science"
      }
    ],
    "year": 1993,
    "topn_sim": null,
    "score": 0.6845604678338282
  },
  {
    "id": "1585375882",
    "title": "Machine learning: ECML-98 : 10th European Conference on Machine Learning, Chemnitz, Germany, April 21-23, 1998 : proceedings",
    "abs": " Learning in agent-oriented worlds.- Naive (Bayes) at forty: The independence assumption in information retrieval.- Learning verbal transitivity using loglinear models.- Part-of-speech tagging using decision trees.- Inference of finite automata: Reducing the search space with an ordering of pairs of states.- Automatic acquisition of lexical knowledge from sparse and noisy data.- A normalization method for contextual data: Experience from a large-scale application.- Learning to classify x-ray images using relational learning.- ILP experiments in detecting traffic problems.- Simulating children learning and explaining elementary heat transfer phenomena: A multistrategy system at work.- Bayes optimal instance-based learning.- Bayesian and information-theoretic priors for Bayesian network parameters.- Feature subset selection in text-learning.- A monotonic measure for optimal feature selection.- Inducing models of human control skills.- God doesn't always shave with Occam's razor - Learning when and how to prune.- Error estimators for pruning regression trees.- Pruning decision trees with misclassification costs.- Text categorization with Support Vector Machines: Learning with many relevant features.- A short note about the application of polynomial kernels with fractional degree in Support Vector Learning.- Classification learning using all rules.- Improved pairwise coupling classification with correcting classifiers.- Experiments on solving multiclass learning problems by n 2-classifier.- Combining classifiers by constructive induction.- Boosting trees for cost-sensitive classifications.- Naive bayesian classifier committees.- Batch classifications with discrete finite mixtures.- Induction of recursive program schemes.- Predicate invention and learning from positive examples only.- An inductive logic programming framework to learn a concept from ambiguous examples.- First-order learning for Web mining.- Explanation-based generalization in game playing: Quantitative results.- Scope classification: An instance-based learning algorithm with a rule-based characterisation.- Error-correcting output codes for local learners.- Recursive lazy learning for modeling and control.- Using lattice-based framework as a tool for feature extraction.- Determining property relevance in concept formation by computing correlation between properties.- A buffering strategy to avoid ordering effects in clustering.- Coevolutionary, distributed search for inducing concept descriptions.- Continuous mimetic evolution.- A host-parasite genetic algorithm for asymmetric tasks.- Speeding up Q(?)-learning.- Q-learning and redundancy reduction in classifier systems with internal state.- Composing functions to speed up reinforcement learning in a changing world.- Theoretical results on reinforcement learning with temporally abstract options.- A general convergence method for Reinforcement Learning in the continuous case.- Interpretable neural networks with BP-SOM.- Convergence rate of minimization learning for neural networks.",
    "n_citation": 1,
    "authors": [
      {
        "name": "Claire Nédellec",
        "id": "2659254252",
        "org": null
      },
      {
        "name": "Céline Rouveirol",
        "id": "2331180646",
        "org": null
      }
    ],
    "year": 1998,
    "topn_sim": null,
    "score": 0.6839114005301855
  },
  {
    "id": "1590183771",
    "title": "Bayesian Reasoning and Machine Learning",
    "abs": " Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.",
    "n_citation": 455,
    "authors": [
      {
        "name": "David Barber",
        "id": "2524043411",
        "org": null
      }
    ],
    "year": 2012,
    "topn_sim": null,
    "score": 0.6838024370742265
  },
  {
    "id": "1590323574",
    "title": "Proceedings of the Sixth International Workshop on Machine Learning, Cornell University, Ithaca, New York, June 26-27, 1989",
    "abs": "",
    "n_citation": 2,
    "authors": [
      {
        "name": "Alberto Maria Segre",
        "id": "2293626924",
        "org": null
      },
      {
        "name": "Jaime G. Carbonell",
        "id": "2100444261",
        "org": null
      },
      {
        "name": "Steve Chien",
        "id": "2124655877",
        "org": null
      },
      {
        "name": "Robotics Program",
        "id": "2162713965",
        "org": null
      }
    ],
    "year": 1989,
    "topn_sim": null,
    "score": 0.6813312858460893
  },
  {
    "id": "1592227380",
    "title": "Machine learning: ECML 2007 : 18th European Conference on Machine Learning, Warsaw, Poland, September 17-21, 2007 : proceedings",
    "abs": " Invited Talks.- Learning, Information Extraction and the Web.- Putting Things in Order: On the Fundamental Role of Ranking in Classification and Probability Estimation.- Mining Queries.- Adventures in Personalized Information Access.- Long Papers.- Statistical Debugging Using Latent Topic Models.- Learning Balls of Strings with Correction Queries.- Neighborhood-Based Local Sensitivity.- Approximating Gaussian Processes with -Matrices.- Learning Metrics Between Tree Structured Data: Application to Image Recognition.- Shrinkage Estimator for Bayesian Network Parameters.- Level Learning Set: A Novel Classifier Based on Active Contour Models.- Learning Partially Observable Markov Models from First Passage Times.- Context Sensitive Paraphrasing with a Global Unsupervised Classifier.- Dual Strategy Active Learning.- Decision Tree Instability and Active Learning.- Constraint Selection by Committee: An Ensemble Approach to Identifying Informative Constraints for Semi-supervised Clustering.- The Cost of Learning Directed Cuts.- Spectral Clustering and Embedding with Hidden Markov Models.- Probabilistic Explanation Based Learning.- Graph-Based Domain Mapping for Transfer Learning in General Games.- Learning to Classify Documents with Only a Small Positive Training Set.- Structure Learning of Probabilistic Relational Models from Incomplete Relational Data.- Stability Based Sparse LSI/PCA: Incorporating Feature Selection in LSI and PCA.- Bayesian Substructure Learning - Approximate Learning of Very Large Network Structures.- Efficient Continuous-Time Reinforcement Learning with Adaptive State Graphs.- Source Separation with Gaussian Process Models.- Discriminative Sequence Labeling by Z-Score Optimization.- Fast Optimization Methods for L1 Regularization: A Comparative Study and Two New Approaches.- Bayesian Inference for Sparse Generalized Linear Models.- Classifier Loss Under Metric Uncertainty.- Additive Groves of Regression Trees.- Efficient Computation of Recursive Principal Component Analysis for Structured Input.- Hinge Rank Loss and the Area Under the ROC Curve.- Clustering Trees with Instance Level Constraints.- On Pairwise Naive Bayes Classifiers.- Separating Precision and Mean in Dirichlet-Enhanced High-Order Markov Models.- Safe Q-Learning on Complete History Spaces.- Random k-Labelsets: An Ensemble Method for Multilabel Classification.- Seeing the Forest Through the Trees: Learning a Comprehensible Model from an Ensemble.- Avoiding Boosting Overfitting by Removing Confusing Samples.- Planning and Learning in Environments with Delayed Feedback.- Analyzing Co-training Style Algorithms.- Policy Gradient Critics.- An Improved Model Selection Heuristic for AUC.- Finding the Right Family: Parent and Child Selection for Averaged One-Dependence Estimators.- Short Papers.- Stepwise Induction of Multi-target Model Trees.- Comparing Rule Measures for Predictive Association Rules.- User Oriented Hierarchical Information Organization and Retrieval.- Learning a Classifier with Very Few Examples: Analogy Based and Knowledge Based Generation of New Examples for Character Recognition.- Weighted Kernel Regression for Predicting Changing Dependencies.- Counter-Example Generation-Based One-Class Classification.- Test-Cost Sensitive Classification Based on Conditioned Loss Functions.- Probabilistic Models for Action-Based Chinese Dependency Parsing.- Learning Directed Probabilistic Logical Models: Ordering-Search Versus Structure-Search.- A Simple Lexicographic Ranker and Probability Estimator.- On Minimizing the Position Error in Label Ranking.- On Phase Transitions in Learning Sparse Networks.- Semi-supervised Collaborative Text Classification.- Learning from Relevant Tasks Only.- An Unsupervised Learning Algorithm for Rank Aggregation.- Ensembles of Multi-Objective Decision Trees.- Kernel-Based Grouping of Histogram Data.- Active Class Selection.- Sequence Labeling with Reinforcement Learning and Ranking Algorithms.- Efficient Pairwise Classification.- Scale-Space Based Weak Regressors for Boosting.- K-Means with Large and Noisy Constraint Sets.- Towards 'Interactive' Active Learning in Multi-view Feature Sets for Information Extraction.- Principal Component Analysis for Large Scale Problems with Lots of Missing Values.- Transfer Learning in Reinforcement Learning Problems Through Partial Policy Recycling.- Class Noise Mitigation Through Instance Weighting.- Optimizing Feature Sets for Structured Data.- Roulette Sampling for Cost-Sensitive Learning.- Modeling Highway Traffic Volumes.- Undercomplete Blind Subspace Deconvolution Via Linear Prediction.- Learning an Outlier-Robust Kalman Filter.- Imitation Learning Using Graphical Models.- Nondeterministic Discretization of Weights Improves Accuracy of Neural Networks.- Semi-definite Manifold Alignment.- General Solution for Supervised Graph Embedding.- Multi-objective Genetic Programming for Multiple Instance Learning.- Exploiting Term, Predicate, and Feature Taxonomies in Propositionalization and Propositional Rule Learning.",
    "n_citation": 1,
    "authors": [
      {
        "name": "Joost N. Kok",
        "id": "2893378956",
        "org": null
      }
    ],
    "year": 2007,
    "topn_sim": null,
    "score": 0.6812692230048012
  },
  {
    "id": "1595055397",
    "title": "Advances in machine learning and cybernetics : 4th International Conference, ICMLC 2005, Guangzhou, China, August 18-21, 2005 : revised selected papers",
    "abs": " Agents and Distributed AI.- Backward-Chaining Flexible Planning.- RRF: A Double-Layer Reputation Mechanism with Rating Reputation Considered.- A Vickrey-Type Multi-attribute Auction Model.- Strategy Coordination Approach for Safe Learning About Novel Filtering Strategies in Multi Agent Framework.- Modeling and Design of Agent Grid Based Open Decision Support System.- Negotiating Agent: Concept, Architecture and Communication Model.- Intelligent Control (I).- Particle Filter Method for a Centralized Multisensor System.- Design and Analysis of a Novel Load-Balancing Model Based on Mobile Agent.- Fuzzy Output Tracking Control and Its Application to Guidance Control for Lunar Gravity-Turn.- Motion Planning for Climbing Robot Based on Hybrid Navigation.- Hierarchical Fuzzy Behavior-Based Control Method for Autonomous Mobile Robot Navigation.- Intelligent Control (II).- An LMI Approach to Robust Fault Detection Filter Design for Uncertain State-Delayed Systems.- Input-to-State Stability Analysis of a Class of Interconnected Nonlinear Systems.- Construction and Simulation of the Movable Propeller Turbine Neural Network Model.- Research on the Control and Application of Chaos in an Electrical System.- Data Mining and Knowledge Discovery (I).- Research and Application of Data Mining in Power Plant Process Control and Optimization.- Extended Negative Association Rules and the Corresponding Mining Algorithm.- An Ant Clustering Method for a Dynamic Database.- An Efficient Algorithm for Incremental Mining of Sequential Patterns.- A Clustering Algorithm Based on Density Kernel Extension.- Associative Classification with Prediction Confidence.- Data Mining and Knowledge Discovery (II).- Exploring Query Matrix for Support Pattern Based Classification Learning.- From Clusters to Rules: A Hybrid Framework for Generalized Symbolic Rule Induction.- Trail-and-Error Approach for Determining the Number of Clusters.- Unifying Genetic Algorithm and Clustering Method for Recognizing Activated fMRI Time Series.- Repeating Pattern Discovery from Audio Stream.- A Study on Information Extraction from PDF Files.- Data Mining and Knowledge Discovery (III).- The Study of a Knowledge-Based Constraints Network System (KCNS) for Concurrent Engineering.- Rule Induction for Complete Information Systems in Knowledge Acquisition and Classification.- A Method to Eliminate Incompatible Knowledge and Equivalence Knowledge.- Constructing Ontologies for Sharing Knowledge in Digital Archives.- A Similarity-Aware Multiagent-Based Web Content Management Scheme.- Information Assistant: An Initiative Topic Search Engine.- Improving Retrieval Performance with the Combination of Thesauri and Automatic Relevance Feedback.- Fuzzy Information Processing (I).- Choquet Integrals with Respect to Fuzzy Measure on Fuzzy ?-Algebra.- Robust H??? Control with Pole Placement Constraints for T-S Fuzzy Systems.- A Kind of Fuzzy Genetic Algorithm Based on Rule and Its Performance Research.- The Absolute Additivity and Fuzzy Additivity of Sugeno Integral.- The Axiomatization for 0-Level Universal Logic.- Fuzzy Portfolio Selection Problems Based on Credibility Theory.- Fuzzy Information Processing (II).- Fuzzy Multiple Reference Models Adaptive Control Scheme Study.- Selection of Optimal Technological Innovation Projects Combining Value Engineering with Fuzzy Synthetic Evaluation.- The Hierarchical Fuzzy Evaluation System and Its Application.- A Solution to a System of Linear Equations with Fuzzy Numbers.- Evolutionary Synthesis of Micromachines Using Supervisory Multiobjective Interactive Evolutionary Computation.- Application of Weighted Ideal Point Method to Environmental/Economic Load Dispatch.- Learning and Reasoning (I).- Reasoning the Spatiotemporal Relations Between Time Evolving Indeterminate Regions.- Using Special Structured Fuzzy Measure to Represent Interaction Among IF-THEN Rules.- Novel Nonlinear Signals Separation of Optimized Entropy Based on Adaptive Natural Gradient Learning.- Training Conditional Random Fields with Unlabeled Data and Limited Number of Labeled Examples.- An Effective and Efficient Two Stage Algorithm for Global Optimization.- Learning and Reasoning (II).- Evolutionary Multi-objective Optimization Algorithm with Preference for Mechanical Design.- A New Adaptive Crossover Operator for the Preservation of Useful Schemata.- Refinement of Fuzzy Production Rules by Using a Fuzzy-Neural Approach.- A Particle Swarm Optimization-Based Approach to Tackling Simulation Optimization of Stochastic, Large-Scale and Complex Systems.- Combination of Multiple Nearest Neighbor Classifiers Based on Feature Subset Clustering Method.- Learning and Reasoning (III).- A Statistical Confidence-Based Adaptive Nearest Neighbor Algorithm for Pattern Classification.- Automatic 3D Motion Synthesis with Time-Striding Hidden Markov Model.- Learning from an Incomplete Information System with Continuous-Valued Attributes by a Rough Set Technique.- Reduction of Attributes in Ordinal Decision Systems.- On the Local Reduction of Information System.- Machine Learning Applications (I).- Spectral Analysis of Protein Sequences.- Volatility Patterns of Industrial Stock Price Indices in the Chinese Stock Market.- Data Migration in RAID Based on Stripe Unit Heat.- Distribution Channel Coordination Through Penalty Schemes.- Automatic Keyphrases Extraction from Document Using Neural Network.- A Novel Fuzzy Anomaly Detection Method Based on Clonal Selection Clustering Algorithm.- Machine Learning Applications (II).- An Anti-worm with Balanced Tree Based Spreading Strategy.- EFIS: Evolvable-Neural-Based Fuzzy Inference System and Its Application for Adaptive Network Anomaly Detection.- Fast Detection of Worm Infection for Large-Scale Networks.- Empirical Study on Fusion Methods Using Ensemble of RBFNN for Network Intrusion Detection.- A Covariance Matrix Based Approach to Internet Anomaly Detection.- Use of Linguistic Features in Context-Sensitive Text Classification.- Machine Learning Applications (III).- Using Term Relationships in a Structured Document Retrieval Model Based on Influence Diagrams.- Kernel-Based Metric Adaptation with Pairwise Constraints.- Generating Personalized Answers by Constructing a Question Situation.- A Multi-stage Chinese Collocation Extraction System.- Monitoring Glaucomatous Progression: Classification of Visual Field Measurements Using Stable Reference Data.- Neural Networks and Statistical Learning Methods (I).- A New Intelligent Diagnostic Method for Machine Maintenance.- Prediction of Human Behaviour Using Artificial Neural Networks.- Iterative Learning Controller for Trajectory Tracking Tasks Based on Experience Database.- MGPC Based on Hopfield Network and Its Application in a Thermal Power Unit Load System.- Neural Networks and Statistical Learning Methods (II).- Solving the Minimum Crossing Number Problem Using an Improved Artificial Neural Network.- The Design of a Fuzzy-Neural Network for Ship Collision Avoidance.- A Genetic Algorithm-Based Neural Network Approach for Fault Diagnosis in Hydraulic Servo-Valves.- Sensitivity Analysis of Madalines to Weight Perturbation.- Neural Networks and Statistical Learning Methods (III).- Fault Diagnosis for the Feedwater Heater System of a 300MW Coal-Fired Power Generating Unit Based on RBF Neural Network.- The Application of Modified Hierarchy Genetic Algorithm Based on Adaptive Niches.- Construction of High Precision RBFNN with Low False Alarm for Detecting Flooding Based Denial of Service Attacks Using Stochastic Sensitivity Measure.- Context-Sensitive Kernel Functions: A Distance Function Viewpoint.- A Parallel Genetic Algorithm for Solving the Inverse Problem of Support Vector Machines.- Neural Networks and Statistical Learning Methods (IV).- Short Term Load Forecasting Model Based on Support Vector Machine.- Evaluation of an Efficient Parallel Object Oriented Platform (EPOOP) for Control Intensive Intelligent Applications.- Location of Tropical Cyclone Center with Intelligent Image Processing Technique.- A Hybrid Genetic Algorithm/Particle Swarm Approach for Evaluation of Power Flow in Electric Network.- Pattern Recognition (I).- A Method to Construct the Mapping to the Feature Space for the Dot Product Kernels.- An Edge Detection Method by Combining Fuzzy Logic and Neural Network.- Fast Face Detection Integrating Motion Energy into a Cascade-Structured Classifier.- Adaptive Online Multi-stroke Sketch Recognition Based on Hidden Markov Model.- To Diagnose a Slight and Incipient Fault in a Power Plant Thermal System Based on Symptom Zoom Technology and Fuzzy Pattern Recognition Method.- Mandarin Voice Conversion Using Tone Codebook Mapping.- Pattern Recognition (II).- Continuous Speech Recognition Based on ICA and Geometrical Learning.- Underwater Target Recognition with Sonar Fingerprint.- Multi-stream Articulator Model with Adaptive Reliability Measure for Audio Visual Speech Recognition.- Optical Font Recognition of Chinese Characters Based on Texture Features.- Some Characteristics of Fuzzy Integrals as a Multiple Classifiers Fusion Method.- Vision and Image Processing (I).- Error Concealment Based on Adaptive MRF-MAP Framework.- Spatial Video Watermarking Based on Stability of DC Coefficients.- A Learning-Based Spatial Processing Method for the Detection of Point Targets.- CTFDP: An Affine Invariant Method for Matching Contours.- A De-noising Algorithm of Infrared Image Contrast Enhancement.- Vision and Image Processing (II).- Speckle Suppressing Based on Fuzzy Generalized Morphological Filter.- Research of Vehicle License Plate Location Algorithm Based on Color Features and Plate Processions.- Fast Measuring Particle Size by Using the Information of Particle Boundary and Shape.- A Statistical Image Fusion Scheme for Multi Focus Applications.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Daniel S. Yeung",
        "id": "2646841985",
        "org": null
      },
      {
        "name": "Zhi-Qiang Liu",
        "id": "2806066953",
        "org": null
      },
      {
        "name": "Xi-Zhao Wang",
        "id": "2305421375",
        "org": null
      },
      {
        "name": "Hong Yan",
        "id": "2676999478",
        "org": null
      }
    ],
    "year": 2006,
    "topn_sim": null,
    "score": 0.6809045954462525
  },
  {
    "id": "1595561882",
    "title": "Machine Learning Usefulness Relies on Accuracy and Self-Maintenance",
    "abs": " A new machine learning system, INNER, is presented in this paper. The system starts out from a collection of training examples; some of them are inflated generalizing their description so as to obtain a first draft of classification rules. An optimization stage, borrowed from our previous system, Fan, is then applied to return the final set of rules. The main goal of Inner, besides its high level of accuracy, is its ability for self-maintenance. To close the paper, we present a number of different experiments carried` out with INNER to illustrate how good the performance and stability of the system is.",
    "n_citation": 5,
    "authors": [
      {
        "name": "Oscar Luaces",
        "id": "2132673682",
        "org": "Centro de Inteligencia Artificial, Universidad de Oviedo en Gijón, Gijón, Spain"
      },
      {
        "name": "J.M. Alonso",
        "id": "2181614180",
        "org": "Centro de Inteligencia Artificial, Universidad de Oviedo en Gijón, Gijón, Spain"
      },
      {
        "name": "Enrique A. de la Cal",
        "id": "2104723287",
        "org": "Centro de Inteligencia Artificial, Universidad de Oviedo en Gijón, Gijón, Spain"
      },
      {
        "name": "José Ranilla",
        "id": "2276163336",
        "org": "Centro de Inteligencia Artificial, Universidad de Oviedo en Gijón, Gijón, Spain"
      },
      {
        "name": "Antonio Bahamonde",
        "id": "2098734601",
        "org": "Centro de Inteligencia Artificial, Universidad de Oviedo en Gijón, Gijón, Spain"
      }
    ],
    "year": 1998,
    "topn_sim": null,
    "score": 0.6806460681528623
  },
  {
    "id": "1596611884",
    "title": "Machine learning : ECML-97 : 9th European Conference on Machine Learning, Prague, Czech Republic, April 23-25, 1997 : proceedings",
    "abs": " Uncertain learning agents.- Constructing and sharing perceptual distinctions.- On prediction by data compression.- Induction of feature terms with INDIE.- Exploiting qualitative knowledge to enhance skill acquisition.- Integrated learning and planning based on truncating temporal differences.- ?-subsumption for structural matching.- Classification by Voting Feature Intervals.- Constructing intermediate concepts by decomposition of real functions.- Conditions for Occam's razor applicability and noise elimination.- Learning different types of new attributes by combining the neural network and iterative attribute construction.- Metrics on terms and clauses.- Learning when negative examples abound.- A model for generalization based on confirmatory induction.- Learning Linear Constraints in Inductive Logic Programming.- Finite-Element methods with local triangulation refinement for continuous reinforcement learning problems.- Inductive Genetic Programming with Decision Trees.- Parallel and distributed search for structure in multivariate time series.- Compression-based pruning of decision lists.- Probabilistic Incremental Program Evolution: Stochastic search through program space.- NeuroLinear: A system for extracting oblique decision rules from neural networks.- Inducing and using decision rules in the GRG knowledge discovery system.- Learning and exploitation do not conflict under minimax optimality.- Model combination in the multiple-data-batches scenario.- Search-based class discretization.- Natural ideal operators in Inductive Logic Programming.- A case study in loyalty and satisfaction research.- Ibots learn genuine team solutions.- Global data analysis and the fragmentation problem in decision tree induction.- Case-based learning: Beyond classification of feature vectors.- Empirical learning of Natural Language Processing tasks.- Human-Agent Interaction and Machine Learning.- Learning in dynamically changing domains: Theory revision and context dependence issues.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Maarten van Someren",
        "id": "2124654629",
        "org": null
      },
      {
        "name": "Gerhard Widmer",
        "id": "1964335307",
        "org": null
      }
    ],
    "year": 1997,
    "topn_sim": null,
    "score": 0.6797539638418312
  },
  {
    "id": "1597150962",
    "title": "Learning Curves in Machine Learning.",
    "abs": "",
    "n_citation": 6,
    "authors": [
      {
        "name": "Claudia Perlich",
        "id": "164824025",
        "org": "IBM"
      }
    ],
    "year": 2010,
    "topn_sim": null,
    "score": 0.6787862375013852
  },
  {
    "id": "1599090857",
    "title": "Machine learning : ECML-95 : 8th European Conference on Machine Learning, Heraclion, Crete, Greece, April 25-27, 1995 : proceedings",
    "abs": " Reasoning and learning in probabilistic and possibilistic networks: An overview.- Problem decomposition and the learning of skills.- Machine learning in the world wide web.- Abstract computer models: Towards a new method for theorizing about adaptive agents.- Learning abstract planning cases.- The role of prototypicality in exemplar-based learning.- Specialization of recursive predicates.- A distributed genetic algorithm improving the generalization behavior of neural networks.- Learning non-monotonic logic programs: Learning exceptions.- A comparative utility analysis of case-based reasoning and control-rule learning systems.- A minimization approach to propositional inductive learning.- On concept space and hypothesis space in case-based learning algorithms.- The power of decision tables.- Pruning multivariate decision trees by hyperplane merging.- Multiple-Knowledge Representations in concept learning.- The effect of numeric features on the scalability of inductive learning programs.- Analogical logic program synthesis from examples.- A guided tour through hypothesis spaces in ILP.- JIGSAW: Puzzling together RUTH and SPECTRE (Extended abstract).- Discovery of constraints and data dependencies in relational databases (Extended abstract).- Learning disjunctive normal forms in a dual classifier system (Extended abstract).- The effects of noise on efficient incremental induction (Extended abstract).- Analysis of Rachmaninoff's piano performances using inductive logic programming (Extended abstract).- Handling real numbers in ILP: A step towards better behavioural clones (Extended abstract).- Simplifying decision trees by pruning and grafting: New results (Extended abstract).- A tight integration of pruning and learning (Extended abstract).- Decision-tree based neural network (Extended abstract).- Learning recursion with iterative bootstrap induction (Extended abstract).- Patching proofs for reuse (Extended abstract).- Adapting to drift in continuous domains (Extended abstract).- Parallel recombinative reinforcement learning (Extended abstract).- Learning to solve complex tasks for reactive systems (Extended abstract).- Co-operative Reinforcement Learning by payoff filters (Extended abstract).- Automatic synthesis of control programs by combination of learning and problem solving methods (Extended abstract).- Analytical learning guided by empirical technology: An approach to integration (Extended abstract).- A new MDL measure for robust rule induction (Extended abstract).- Class-driven statistical discretization of continuous attributes (Extended abstract).- Generating neural networks through the induction of threshold logic unit trees (Extended abstract).- Learning classification rules using lattices (Extended abstract).- Hybrid classification: Using axis-parallel and oblique subdivisions of the attribute space (Extended abstract).- An induction-based control for genetic algorithms (Extended abstract).- Fender: An approach to theory restructuring (extended abstract).- Language series revisited: The complexity of hypothesis spaces in ILP (Extended abstract).- Prototype, nearest neighbor and hybrid algorithms for time series classification (Extended abstract).",
    "n_citation": 0,
    "authors": [
      {
        "name": "Nada Lavrač",
        "id": "44897255",
        "org": null
      },
      {
        "name": "Stefan Wrobel",
        "id": "2113978936",
        "org": null
      }
    ],
    "year": 1995,
    "topn_sim": null,
    "score": 0.6781317052487734
  },
  {
    "id": "1599262381",
    "title": "Machine learning : ECML 2003 : 14th European Conference on Machine Learning, Cavtat-Dubrovnik, Croatia, September 22-26, 2003 : proceedings",
    "abs": " Invited Papers.- From Knowledge-Based to Skill-Based Systems: Sailing as a Machine Learning Challenge.- Two-Eyed Algorithms and Problems.- Next Generation Data Mining Tools: Power Laws and Self-similarity for Graphs, Streams and Traditional Data.- Taking Causality Seriously: Propensity Score Methodology Applied to Estimate the Effects of Marketing Interventions.- Contributed Papers.- Support Vector Machines with Example Dependent Costs.- Abalearn: A Risk-Sensitive Approach to Self-play Learning in Abalone.- Life Cycle Modeling of News Events Using Aging Theory.- Unambiguous Automata Inference by Means of State-Merging Methods.- Could Active Perception Aid Navigation of Partially Observable Grid Worlds?.- Combined Optimization of Feature Selection and Algorithm Parameters in Machine Learning of Language.- Iteratively Extending Time Horizon Reinforcement Learning.- Volume under the ROC Surface for Multi-class Problems.- Improving the AUC of Probabilistic Estimation Trees.- Scaled CGEM: A Fast Accelerated EM.- Pairwise Preference Learning and Ranking.- A New Way to Introduce Knowledge into Reinforcement Learning.- Improvement of the State Merging Rule on Noisy Data in Probabilistic Grammatical Inference.- COllective INtelligence with Sequences of Actions.- Rademacher Penalization over Decision Tree Prunings.- Learning Rules to Improve a Machine Translation System.- Optimising Performance of Competing Search Engines in Heterogeneous Web Environments.- Robust k-DNF Learning via Inductive Belief Merging.- Logistic Model Trees.- Color Image Segmentation: Kernel Do the Feature Space.- Evaluation of Topographic Clustering and Its Kernelization.- A New Pairwise Ensemble Approach for Text Classification.- Self-evaluated Learning Agent in Multiple State Games.- Classification Approach towards Ranking and Sorting Problems.- Using MDP Characteristics to Guide Exploration in Reinforcement Learning.- Experiments with Cost-Sensitive Feature Evaluation.- A Markov Network Based Factorized Distribution Algorithm for Optimization.- On Boosting Improvement: Error Reduction and Convergence Speed-Up.- Improving SVM Text Classification Performance through Threshold Adjustment.- Backoff Parameter Estimation for the DOP Model.- Improving Numerical Prediction with Qualitative Constraints.- A Generative Model for Semantic Role Labeling.- Optimizing Local Probability Models for Statistical Parsing.- Extended Replicator Dynamics as a Key to Reinforcement Learning in Multi-agent Systems.- Visualizations for Assessing Convergence and Mixing of MCMC.- A Decomposition of Classes via Clustering to Explain and Improve Naive Bayes.- Improving Rocchio with Weakly Supervised Clustering.- A Two-Level Learning Method for Generalized Multi-instance Problems.- Clustering in Knowledge Embedded Space.- Ensembles of Multi-instance Learners.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Nada Lavrač",
        "id": "2627797131",
        "org": null
      }
    ],
    "year": 2003,
    "topn_sim": null,
    "score": 0.6766634939544612
  },
  {
    "id": "1600840636",
    "title": "Some Notes on Applied Mathematics for Machine Learning",
    "abs": " This chapter describes Lagrange multipliers and some selected subtopics from matrix analysis from a machine learning perspective. The goal is to give a detailed description of a number of mathematical constructions that are widely used in applied machine learning.",
    "n_citation": 4,
    "authors": [
      {
        "name": "Christopher J. C. Burges",
        "id": "2014469688",
        "org": "Microsoft Research"
      }
    ],
    "year": 2003,
    "topn_sim": null,
    "score": 0.6764743667133742
  },
  {
    "id": "1600923755",
    "title": "Neural networks and machine learning: towards fully automated learning",
    "abs": " Neural networks and machine learning algorithms (hereafter NN) represent a dramatic departure from conventional programming techniques. Rather than explicitly build a program to solve a problem, one presents examples (training set) of a particular application to the NN and the NN adjusts itself in a self-organizing fashion to solve the training set. It can then be presented new examples on which it was not trained (test set) and the NN will generalize to give a good answer. For example, a doctor goes through years of training to learn to diagnose disease on the basis of a set of symptoms. An NN would solve this problem by first learning a training set made up of symptoms with the correct diagnoses, and then generalizing to a diagnosis when presented with new symptoms on which it was not trained. For many tasks the NN approaches outperform human experts and other automated techniques. This general approach is amenable to many difficult problems being faced today. This talk will review basic approaches in machine learning algorithms and then discuss latest challenges and goals. In particular, I will discuss the challenge of fully automated learning which allows minimal input from a user and requires the machine learning model to automatically discover how best to learn the problem. This includes not only deciding on features and learning parameters, but also the automatic selection of an inductive bias which will allow the learning model to achieve the highest possible accuracy.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Tony R. Martinez",
        "id": "2145260682",
        "org": "Brigham Young University"
      }
    ],
    "year": 2004,
    "topn_sim": null,
    "score": 0.6764665955450886
  },
  {
    "id": "160128500",
    "title": "Inferential Theory of Learning as a Conceptual Basis for Machine Learning",
    "abs": "",
    "n_citation": 2,
    "authors": [
      {
        "name": "Ryszard Michalski",
        "id": "2617088048",
        "org": null
      }
    ],
    "year": 1993,
    "topn_sim": null,
    "score": 0.67588106553052
  },
  {
    "id": "1601899527",
    "title": "Towards a technology and a science of machine learning",
    "abs": "",
    "n_citation": 12,
    "authors": [
      {
        "name": "Derek H. Sleeman",
        "id": "697203324",
        "org": null
      }
    ],
    "year": 1994,
    "topn_sim": null,
    "score": 0.67443934601525
  },
  {
    "id": "1604098463",
    "title": "Towards intelligent machine learning algorithms",
    "abs": " Abstract : Machine learning is recognized as a tool for improving the performance of many kinds of systems, yet most machine learning systems themselves are not well equipped to improve their own learning performance. By emphasizing the role of domain knowledge, learning systems can be crafted as knowledge directed systems, and with the addition of a knowledge store for organizing and maintaining knowledge to assist learning, a learning machine learning (L-ML) algorithm is possible. The necessary components of L-ML systems are presented along with several case descriptions of existing machine learning systems that possess limited L-ML capabilities. Keywords: Algorithms, Artificial intelligence. (SDW)",
    "n_citation": 7,
    "authors": [
      {
        "name": "Robert E. Stepp",
        "id": "2131634145",
        "org": null
      },
      {
        "name": "Bradley L. Whitehall",
        "id": "2578637560",
        "org": null
      },
      {
        "name": "Lawrence B. Holder",
        "id": "2657443039",
        "org": null
      }
    ],
    "year": 1988,
    "topn_sim": null,
    "score": 0.6739574322802836
  },
  {
    "id": "1607090429",
    "title": "From Machine Learning to Child Learning",
    "abs": " Machine Learning endeavors to make computers learn and improve themselves over time. It is originated from analyzing human learning, and is now maturing as computers can learn more effectively than human for many specific tasks, such as adaptive expert systems and data mining. The effective and fruitful research in machine learning can now be used to improve our thinking and learning, especially for our children. In this talk, I will discuss my efforts in using machine learning (and AI) for child education in Canada and China. In early 2009, I hosted a TV series in a major talk show in China. The impact of such work in China and around the world can be huge.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Charles X. Ling",
        "id": "1980182462",
        "org": "Department of Computer Science, University of Western Ontario, Canada#TAB#"
      }
    ],
    "year": 2009,
    "topn_sim": null,
    "score": 0.6733252724722305
  },
  {
    "id": "1608143920",
    "title": "Function decomposition in machine learning",
    "abs": " To solve a complex problem, one of the effective general approaches is to decompose it into smaller, less complex and more manageable subproblems. In machine learning, this principle is a foundation for structured induction [44]: instead of learning a single complex classification rule from examples, define a concept hierarchy and learn rules for each of the (sub)concepts. Shapiro [44] used structured induction for the classification of a fairly complex chess endgame and demonstrated that the complexity and comprehensiveness (“brain-compatibility”) of the obtained solution was superior to the unstructured one. Shapiro was helped by a chess master to structure his problem domain. Typically, applications of structured induction involve a manual development of the hierarchy and a manual selection and classification of examples to induce the subconcept classification rules; usually this is a tiresome process that requires an active availability of a domain expert over long periods of time. Therefore, it would be very desirable to automate the problem decomposition task.",
    "n_citation": 11,
    "authors": [
      {
        "name": "Blaz Zupan",
        "id": "2030178555",
        "org": "Univ. of Ljubljana, and J. Stefan Institute, Ljubljana, Slovenia, and Baylor College of Medicine, Houston, TX#TAB#"
      },
      {
        "name": "Ivan Bratko",
        "id": "689465770",
        "org": "Univ. of Ljubljana and J. Stefan Institute, Ljubljana, Slovenia#TAB#"
      },
      {
        "name": "Marko Bohanec",
        "id": "624460926",
        "org": "J. Stefan Institute, Ljubljana, Slovenia#TAB#"
      },
      {
        "name": "Janez Demšar",
        "id": "1965088163",
        "org": "Univ. of Ljubljana, Ljubljana, Slovenia#TAB#"
      }
    ],
    "year": 2001,
    "topn_sim": null,
    "score": 0.6732191013782355
  },
  {
    "id": "162414109",
    "title": "Probabilistic Performance Evaluation for Multiclass Classification Using the Posterior Balanced Accuracy",
    "abs": " An important problem in robotics is the empirical evaluation of classification algorithms that allow a robotic system to make accurate categorical predictions about its environment. Current algorithms are often assessed using sample statistics that can be difficult to interpret correctly and do not always provide a principled way of comparing competing algorithms. In this paper, we present a probabilistic alternative based on a Bayesian framework for inferring on balanced accuracies. Using the proposed probabilistic evaluation, it is possible to assess the balanced accuracy’s posterior distribution of binary and multiclass classifiers. In addition, competing classifiers can be compared based on their respective posterior distributions. We illustrate the practical utility of our scheme and its properties by reanalyzing the performance of a recently published algorithm in the domain of visual action detection and on synthetic data. To facilitate its use, we provide an open-source MATLAB implementation.",
    "n_citation": 12,
    "authors": [
      {
        "name": "Henry Carrillo",
        "id": "2119339680",
        "org": "Universidad de Zaragoza"
      },
      {
        "name": "Kay Henning Brodersen",
        "id": "2591070385",
        "org": "Swiss Federal Institute of Technology (ETH Zurich)"
      },
      {
        "name": "José A. Castellanos",
        "id": "2139035419",
        "org": "Universidad de Zaragoza"
      }
    ],
    "year": 2014,
    "topn_sim": null,
    "score": 0.6729665334946595
  },
  {
    "id": "1646316203",
    "title": "Considerations upon the Machine Learning Technologies",
    "abs": " Artificial intelligence offers superior techniques and methods by which problems from diverse domains may find an optimal solution. The Machine Learning technologies refer to the domain of artificial intelligence aiming to develop the techniques allowing the computers to \"learn\". Some systems based on Machine Learning technologies tend to eliminate the necessity of the human intelligence while the others adopt a man-machine collaborative approach.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Alin Munteanu",
        "id": "2020524667",
        "org": null
      },
      {
        "name": "Cristina Ofelia Sofran",
        "id": "2339743795",
        "org": null
      }
    ],
    "year": 2009,
    "topn_sim": null,
    "score": 0.6708068290950282
  },
  {
    "id": "1670901425",
    "title": "Introduction to Machine Learning: Class Notes 67577",
    "abs": " Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).",
    "n_citation": 4,
    "authors": [
      {
        "name": "Amnon Shashua",
        "id": "130468041",
        "org": "School of Computer Science and Engineering|Hebrew University of Jerusalem"
      }
    ],
    "year": 2009,
    "topn_sim": null,
    "score": 0.6707381315396834
  },
  {
    "id": "1674145067",
    "title": "Estimating the accuracy of learned concepts",
    "abs": " This paper investigates alternative estimators of the accuracy of concepts learned from examples. In particular, the cross-validation and 632 bootstrap estimators are studied, using synthetic training data and the FOIL learning algorithm. Our experimental results contradict previous papers in statistics, which advocate the 632 bootstrap method as superior to cross-validation. Nevertheless, our results also suggest that conclusions based on cross-validation in previous machine learning papers are unreliable. Specifically, our observations are that (i) the true error of the concept learned by FOIL from independently drawn sets of examples of the same concept varies widely, (ii) the estimate of true error provided by cross-validation has high variability but is approximately unbiased, and (iii) the 632 bootstrap estimator has lower variability than cross-validation, but is systematically biased.",
    "n_citation": 29,
    "authors": [
      {
        "name": "Timothy L. Bailey",
        "id": "2096884658",
        "org": "Department of Computer Science and Engineering, University of California, San Diego, La Jolla, California#TAB#"
      },
      {
        "name": "Charles Elkan",
        "id": "705676171",
        "org": "Department of Computer Science and Engineering, University of California, San Diego, La Jolla, California#TAB#"
      }
    ],
    "year": 1993,
    "topn_sim": null,
    "score": 0.6700129025318688
  },
  {
    "id": "1680640252",
    "title": "A tour of machine learning: An AI perspective",
    "abs": " Machine Learning has been at the core of Artificial Intelligence since its inception. Many promises have been held, if one is to consider that Google is a living demonstration of AI. This paper presents a historical perspective on Machine Learning, describing how the emphasis was gradually shifted from logical to statistical induction, from induction to optimization, from the search of hypotheses to the search of representations. The paper concludes with a discussion about the new frontier of Machine Learning.",
    "n_citation": 7,
    "authors": [
      {
        "name": "Michèle Sebag",
        "id": "2127154967",
        "org": "TAO, CNRS--INRIA--LRI, Université Paris-Sud, 91405 Orsay Cedex, France. E-mail: sebag@lri.fr#TAB#"
      }
    ],
    "year": 2014,
    "topn_sim": null,
    "score": 0.6693905847814312
  },
  {
    "id": "1680797894",
    "title": "Encyclopedia of Machine Learning",
    "abs": " This comprehensive encyclopedia, with over 250 entries in an A-Z format, provides easy access to relevant information for those seeking entry into any aspect within the broad field of machine learning. Most entries in this preeminent work include useful literature references.Topics for the Encyclopedia of Machine Learning were selected by a distinguished international advisory board. These peer-reviewed, highly-structured entries include definitions, illustrations, applications, bibliographies and links to related literature, providing the reader with a portal to more detailed information on any given topic.The style of the entries in the Encyclopedia of Machine Learning is expository and tutorial, making the book a practical resource for machine learning experts, as well as professionals in other fields who need to access this vital information but may not have the time to work their way through an entire text on their topic of interest.The authoritative reference is published both in print and online. The print publication includes an index of subjects and authors. The online edition supplements this index with hyperlinks as well as internal hyperlinks to related entries in the text, CrossRef citations, and links to additional significant research.",
    "n_citation": 208,
    "authors": [
      {
        "name": "Claude Sammut",
        "id": "2081312962",
        "org": null
      },
      {
        "name": "Geoffrey I. Webb",
        "id": "2810962386",
        "org": null
      }
    ],
    "year": 2011,
    "topn_sim": null,
    "score": 0.6687597583591061
  },
  {
    "id": "170723506",
    "title": "Machine Learning in the Next Five Years.",
    "abs": "",
    "n_citation": 26,
    "authors": [
      {
        "name": "Donald Michie",
        "id": "2163634985",
        "org": null
      }
    ],
    "year": 1988,
    "topn_sim": null,
    "score": 0.6686333415205121
  },
  {
    "id": "171594045",
    "title": "The Effects of Data Quality on Machine Learning Algorithms.",
    "abs": "",
    "n_citation": 9,
    "authors": [
      {
        "name": "Valerie Sessions",
        "id": "2399499206",
        "org": null
      },
      {
        "name": "Marco Valtorta",
        "id": "2007205120",
        "org": null
      }
    ],
    "year": 2006,
    "topn_sim": null,
    "score": 0.6683663691639213
  },
  {
    "id": "172977969",
    "title": "Machine Learning Meets Natural Language",
    "abs": "",
    "n_citation": 2,
    "authors": [
      {
        "name": "Tom M. Mitchell",
        "id": "2151014374",
        "org": null
      }
    ],
    "year": 1997,
    "topn_sim": null,
    "score": 0.6682927448800842
  },
  {
    "id": "174941419",
    "title": "International Conference on Machine Learning (ICML-99).",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Tobias Scheffer",
        "id": "2649079182",
        "org": null
      }
    ],
    "year": 1999,
    "topn_sim": null,
    "score": 0.6680727208376679
  },
  {
    "id": "174998605",
    "title": "Editorial: Advances in artificial neural networks and machine learning",
    "abs": " This work aims at a reflection on the evolution of the field of Neurocomputing along the last 20 years that have witnessed the sequence of editions of the International Work-Conference on Artificial Neural Networks (IWANN). This reflection arises inextricably of the evolution of connectionist networks themselves, describing their features and most remarkable particularities, most of which have prevailed in time. Another trend that is worth mentioning is the development of a strong interconnection with other paradigms comprised under the so-called Computational Intelligence, which can be understood as a set of nature-inspired computational methodologies and approaches to address complex real-world problems, which traditional approaches are ineffective or unfeasible to deal with. Indeed, many hybrid computational intelligence schemes have been developed that efficiently combine procedures from the domains of artificial neural networks, machine learning, evolutionary computation and fuzzy logic to be applied in complex domains. Finally, a brief description of the diverse contributions that have been included in this special issue is presented. These papers stem from previous versions presented at IWANN2011.",
    "n_citation": 6,
    "authors": [
      {
        "name": "Alberto Prieto",
        "id": "2162135802",
        "org": "CITIC-UGR, University of Granada, Computer Architecture, C/ Periodista Daniel Saucedo s/n, 18071 Granada, Spain#TAB#"
      },
      {
        "name": "Miguel Atencia",
        "id": "2034629753",
        "org": "Universidad de Málaga, Spain#TAB#"
      },
      {
        "name": "F. Sandoval",
        "id": "2119311098",
        "org": "Universidad de Málaga, Spain#TAB#"
      }
    ],
    "year": 2013,
    "topn_sim": null,
    "score": 0.6676102674247676
  },
  {
    "id": "1754293002",
    "title": "Machine Learning that Matters",
    "abs": " Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.",
    "n_citation": 77,
    "authors": [
      {
        "name": "Kiri L. Wagstaff",
        "id": "2110892363",
        "org": "Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109 USA"
      }
    ],
    "year": 2012,
    "topn_sim": null,
    "score": 0.6667383649212454
  },
  {
    "id": "1756061918",
    "title": "Machine learning for robotics",
    "abs": " Robots are typically far less capable in autonomous mode than in tele-operated mode. The few exceptions tend to stem from long days (and more often weeks, or even years) of expert engineering for a specific robot and its operating environment. Current control methodology is quite slow and labor intensive. I believe advances in machine learning have the potential to revolutionize robotics. In this talk, I will present new machine learning techniques we have developed that are tailored to robotics. I will describe in depth \"Apprenticeship learning\", a new approach to high-performance robot control based on learning for control from ensembles of expert human demonstrations. Our initial work in apprenticeship learning has enabled the most advanced helicopter aerobatics to-date, including maneuvers such as chaos, tic-tocs, and auto-rotation landings which only exceptional expert human pilots can fly. Our most recent work in apprenticeship learning is providing traction on learning to perform challenging robotic manipulation tasks, such as knot-tying. I will also briefly highlight three other machine learning for robotics developments: Inverse reinforcement learning and its application to quadruped locomotion, Safe exploration in reinforcement learning which enables robots to learn on their own, and Learning for perception with application to robotic laundry.",
    "n_citation": 0,
    "authors": [
      {
        "name": "Pieter Abbeel",
        "id": "243981275",
        "org": "University of California, Berkeley#TAB#"
      }
    ],
    "year": 2012,
    "topn_sim": null,
    "score": 0.6659493160813914
  },
  {
    "id": "1770975",
    "title": "Experiments in machine learning and thinking.",
    "abs": "",
    "n_citation": 10,
    "authors": [
      {
        "name": "Tom Kilburn",
        "id": "2790304572",
        "org": null
      },
      {
        "name": "R. L. Grimsdale",
        "id": "2714083038",
        "org": null
      },
      {
        "name": "Frank H. Sumner",
        "id": "2109888865",
        "org": null
      }
    ],
    "year": 1959,
    "topn_sim": null,
    "score": 0.6651029514528769
  },
  {
    "id": "1787685772",
    "title": "The Philosophy of Science and its relation to Machine Learning",
    "abs": " In this chapter I discuss connections between machine learning and the philosophy of science. First I consider the relationship between the two disciplines. There is a clear analogy between hypothesis choice in science and model selection in machine learning. While this analogy has been invoked to argue that the two disciplines are essentially doing the same thing and should merge, I maintain that the disciplines are distinct but related and that there is a dynamic interaction operating between the two: a series of mutually beneficial interactions that changes over time. I will introduce some particularly fruitful interactions, in particular the consequences of automated scientific discovery for the debate on inductivism versus falsificationism in the philosophy of science, and the importance of philosophical work on Bayesian epistemology and causality for contemporary machine learning. I will close by suggesting the locus of a possible future interaction: evidence integration.",
    "n_citation": 7,
    "authors": [
      {
        "name": "Jon Williamson",
        "id": "2432408387",
        "org": "University of Kent"
      }
    ],
    "year": 2009,
    "topn_sim": null,
    "score": 0.6644562200448603
  },
  {
    "id": "1797662972",
    "title": "Representation change in machine learning",
    "abs": "",
    "n_citation": 2,
    "authors": [
      {
        "name": "Lorenza Saitta",
        "id": "2443250341",
        "org": null
      }
    ],
    "year": 1996,
    "topn_sim": null,
    "score": 0.6640504479623934
  },
  {
    "id": "179927012",
    "title": "Some principles of artificial learning that have emerged from examples",
    "abs": " We argue that A. I. should not lose sight of the need for general principles when working with problems in specific domains. We also argue the case for studying Artificial Learning. We present a computer program with a very limited sense modality that acquires some facility with the English language and learns some numerical concepts. The principles by which such learning takes place are expressed in terms of a concept of process and they prove to be applicable to learning the decimal numeral system and forming elementary utterances as well as learning to interpret English sentences.",
    "n_citation": 1,
    "authors": [
      {
        "name": "John Knapman",
        "id": "2284606141",
        "org": "Department of Artificial Intelligence, University of Edinburgh#TAB#"
      }
    ],
    "year": 1975,
    "topn_sim": null,
    "score": 0.663885176449217
  },
  {
    "id": "1852554815",
    "title": "Proceedings of the 4th Workshop on Machine Learning for Interactive Systems (MLIS-2015)",
    "abs": "",
    "n_citation": 0,
    "authors": [
      {
        "name": "Heriberto Cuayáhuitl",
        "id": "2796502881",
        "org": null
      },
      {
        "name": "Nina Dethlefs",
        "id": "1964698620",
        "org": null
      },
      {
        "name": "Lutz Frommberger",
        "id": "163492721",
        "org": null
      },
      {
        "name": "Martijn van Otterlo",
        "id": "2289582208",
        "org": null
      },
      {
        "name": "Olivier Pietquin",
        "id": "175821849",
        "org": null
      }
    ],
    "year": 2015,
    "topn_sim": null,
    "score": 0.6637988778846415
  },
  {
    "id": "185531298",
    "title": "Machine learning: principles and techniques: Richard Forsyth (ed) Chapman and Hall Ltd, UK (1989) £18.50, ISBN 0412305801, 255pp",
    "abs": "",
    "n_citation": 2,
    "authors": [
      {
        "name": "Donald Michie",
        "id": "2163634985",
        "org": "The Turing Institute, UK#TAB#"
      }
    ],
    "year": 1990,
    "topn_sim": null,
    "score": 0.6637852869337924
  }
];